# บทที่ 9 </br> การประยุกต์ใช้โมเดลการประมวลผลภาษาธรรมชาติแบบสำเร็จรูป

```{admonition} จุดมุ่งหมายของบทนี้
- ผู้อ่านเข้าใจและสามารถอธิบายหลักการของงานด้านการประมวลผลภาษาธรรมชาติและการนำไปใช้ได้ เช่น การรู้จำเอนทิตี การวิเคราะห์อารมณ์ความรู้สึก โมเดลหัวเรื่อง เป็นต้น
- ผู้อ่านสามารถจำลองโมเดลการประมวลผลภาษาธรรมชาติได้
- 
```
การประมวลผลภาษาธรรมชาติ คือ เทคนิควิธีที่ใช้ในการประมวลผลและทำความเข้าใจข้อมูลตัวอักษร โดยอาศัยโมเดลทางภาษา โมเดลหรือโมเดลที่เราพูดถึงนี้หมายถึง การเขียนโปรแกรมที่จำลองการทำความเข้าใจภาษาในด้านต่าง ๆ ซึ่งการจำลองอาจจะไม่ได้สอดคล้องกับวิธีที่มนุษย์ทำความเข้าใจภาษา แต่ว่าโมเดลจำลองพฤติกรรมการประมวลผลภาษาได้ดีพอ ที่จะนำไปใช้ในการทำงานที่เกี่ยวข้องกับภาษาธรรมชาติได้ 
งานที่สามารถใช้โมเดลการประมวลผลภาษาธรรมชาติได้อย่างมีประสิทธิภาพ และพบเห็นได้บ่อย ๆ ได้แก่ การติดป้ายกำกับชนิดของคำ (part-of-speech tagging) การตรวจจับเอนติตี้ (named-entity recognition) การวิเคราะห์อารมณ์ความรู้สึก (sentiment analysis) โมเดลหัวเรื่อง (topic model) การแปลด้วยเครื่อง (machine translation) ซึ่งทั้งหมดนี้คือต่างใช้การเรียนรู้ของเครื่อง (machine learning) ในการสร้างโมเดลสำหรับงานเหล่านี้ขึ้นมา กล่าวคือเราเขียนโปรแกรมที่เรียนรู้การทำงานเหล่านี้โดยการหาแพทเทิร์นจากชุดข้อมูลที่เหมาะสม เราไม่ได้ตั้งโปรแกรมให้ทำงานตามกฎของภาษาศาสตร์โดยตรง 

ในปัจจุบันเราสามารถดาวน์โหลดโมเดลและนำมาประยุกต์ใช้ได้ทันทีโดยที่ต้องไม่ทำการสร้างโมเดลใหม่ตั้งแต่ต้น โมเดลเหล่านี้เราเรียกว่า โมเดลการประมวลผลภาษาธรรมชาติแบบสำเร็จรูป (off-the-shelf) แต่ว่าอาจจะมีโมเดลสำหรับบางงานและบางภาษาเท่านั้น และโมเดลของแต่ละภาษามีประสิทธิภาพ และความแม่นยำแตกต่างกันไป ในบทนี้เราจะเรียนรู้วิธีการใช้ไลบรารีเพื่อดาวน์โหลดและประยุกต์ใช้โมเดลในการวิเคราะห์ข้อมูลภาษาโดยอัตโนมัติ

## การสร้างโมเดลประมวลผลภาษาธรรมชาติโดยอาศัยการเรียนรู้ของเครื่อง 

โมเดลประมวลผลภาษาธรรมชาติปัจจุบันสามารถแบ่งออกมาได้เป็นสองประเภทใหญ่ ๆ ได้แก่ 

### 1) โมเดลที่ทำงานได้เฉพาะเจาะจง
โมเดลประเภทนี้เป็นโมเดลที่สร้างขึ้นมาเพื่อทำงานได้เพียงแค่หนึ่งอย่าง เช่น โมเดลติดป้ายกำกับชนิดของคำสามารถประมวลผลข้อความเพื่อวิเคราะห์ชนิดคำของคำแต่ละคำในข้อความได้เพียงอย่างเดียวเท่านั้น ไม่สามารถทำงานประเภทอื่น ๆ ได้  อีกหนึ่งตัวอย่างโมเดลที่ทำงานได้เฉพาะเจาะจงได้เห็นไปแล้วในบทที่แล้วคือ โมเดลตัดคำภาษาไทย ซึ่งสามารถทำได้เพียงหนึ่งอย่างเท่านั้น หากเราต้องการติดป้ายกำกับชนิดของคำ เราจำเป็นต้องใช้โมเดลตัดคำภาษาไทยเพื่อแปลงข้อความให้เป็นลิสต์ของคำให้เรียบร้อยก่อนที่นำไปป้อนให้โมเดลติดป้ายกำกับชนิดของคำต่อไป เพราะฉะนั้นจึงจำเป็นต้องใช้โมเดลสองโมเดล แต่ละโมเดลที่หน้าที่ต่างกันตามแต่การใช้งาน 

โมเดลประเภทนี้เป็นโมเดลที่มักจะอาศัยการเรียนรู้แบบมีผู้สอน (supervised learning) กล่าวคือต้องมีการสร้างชุดข้อมูลเพื่อฝึกฝนโมเดลให้ทำงานที่ต้องการ เช่น หากต้องการสร้างโมเดลติดป้ายกำกับชนิดของคำ ต้องสร้างชุดข้อมูลที่มีตัวอย่างการติดป้ายกำกับชนิดของคำอย่างถูกต้องขึ้นมาก่อน เพื่อให้โมเดลได้เรียนรู้การทำงานนี้จากชุดข้อมูลชุดนี้ โมเดลการเรียนรู้ของเครื่องประเภทนี้มีความสามารถในการจับหาแพทเทิร์นในข้อมูลภาษาที่อยู่ในชุดข้อมูล แล้วกลั่นเก็บออกมาในรูปของตัวเลขซึ่งเรียกว่าพารามิเตอร์ของโมเดล (model parameter) กระบวนการนี้เรียกว่ากระบวนการฝึกโมเดล (model training) จากชุดฝึกฝน (training set) วิธีการทำงานและฝึกฝนโมเดลมักจะเริ่มจากการแปลงภาษาให้เป็นตัวเลขที่มีประโยชน์ในการคำนวณและวิเคราะห์ภาษา แต่ละโมเดลจะมีวิธีการแปลงภาษาเป็นตัวเลขที่ต่างกันไป ตัวอย่างเช่น โมเดลการถดถอยโลจิสติก (logistic regression model) อาจมีการแปลงข้อมูลที่ต้องการวิเคราะห์ให้เป็นตัวเลข โดยการนับความถี่ของคำแต่ละคำที่ปรากฏอยู่ในข้อความ จากนั้นนำความถี่ (ซึ่งเป็นข้อมูลที่เป็นตัวเลขล้วน ๆ ไม่มีสตริงเข้ามาเป็นส่วนเกี่ยวข้องอีกต่อไป) เข้าสู่สมการที่สามารถพยากรณ์ได้ว่าคำแต่คำจัดเป็นคำประเภทใด รายละเอียดสมการของโมเดลที่ใช้ในการประมวลผลภาษาธรรมชาตินั้นอยู่เกินขอบเขตของหนังสือเล่มนี้

การพัฒนาโมเดลการประมวลภาษาธรรมชาติมักจะอาศัยการประเมินความแม่นยำของโมเดล เพื่อจะได้ทราบว่าโมเดลใดมีความแม่นยำสูงสุดในงานใดบ้าง  ทำให้ง่ายต่อการพัฒนาและเลือกใช้โมเดลได้อย่างเหมาะสม ซึ่งมักจะแยกประเมินความแม่นยำของแต่ละงานไป โมเดลที่แม่นยำที่สุดของแต่ละงานมักจะถูกเรียกว่าเป็นโมเดลที่ทันสมัยที่สุด (State-of-the-art model หรือ SOTA) เรามักจะไม่เรียกว่า “โมเดลที่ดีที่สุด” เนื่องจากโมเดลเหล่านี้เป็นเพียงโมเดลที่แม่นยำที่สุดเท่าที่ถูกค้นพบ พัฒนา และทดสอบในช่วงขณะที่กำลังอ้างถึง เพื่อหลีกเลี่ยงการอ้างว่าเป็นโมเดลที่ดีที่สุดเท่าที่จะพึงมีได้ โมเดลที่ทันสมัยที่สุดของแต่ละงานการประมวลผลภาษาธรรมชาติโดยทั่วไปแล้วมักจะเป็นโมเดลที่ทำงานได้เฉพาะเจาะจง และอาศัยการเรียนรู้แบบมีผู้สอน เนื่องจากเป็นการพัฒนาเพื่อมุ่งเน้นเพียงจุดประสงค์เดียว และโมเดลเหล่านี้มักจะมีความสามารถในการเรียนรู้เรื่องที่เจาะจงจากข้อมูลปริมาณมากได้อย่างมีประสิทธิภาพ กล่าวคือพยายามตรวจจับหาแพทเทิร์นที่มีประโยชน์ให้ได้มากที่สุดตามแต่ปริมาณข้อมูลที่อยู่ในชุดฝึกฝน 
เนื้อหาในบทนี้จะเน้นโมเดลประเภทนี้สำหรับการประมวลภาษาไทย โมเดลบางประเภทที่แนะนำในบทนี้อาจจะไม่ใช่โมเดลที่ทันสมัยที่สุด หากแต่เป็นโมเดลสำเร็จรูปที่สามารถนำไปใช้งานได้ง่าย และความแม่นยำไม่ได้ห่างจากโมเดลที่ทันสมัยที่สุดมากนัก ทั้งนี้ผู้เขียนโปรแกรมควรทราบว่าโมเดลที่นำมาใช้โดยผ่านไลบรารีนั้นมีความแม่นยำเพียงใด หรือมีการประเมินผลโดยอิงชุดข้อมูลที่ได้รับการยอมรับว่ามีคุณภาพดีหรือไม่ 
 
### 2) โมเดลที่ทำงานได้หลากหลาย 

ปัจจุบันนี้โมเดลภาษาขนาดใหญ่ (Large Language Model) เช่น GPT4 และ Gemini ได้เข้ามีบทบาทอย่างมากในการประมวลผลภาษาธรรมชาติ เนื่องจากเป็นโมเดลที่ถูกพัฒนาขึ้นมาเพื่อให้ทำงานได้อย่างหลากหลาย และได้ถูกฝึกฝนบนชุดข้อมูลตัวอย่างงานการประมวลภาษาธรรมชาติโดยเฉพาะอีกด้วย เช่น การติดป้ายกำกับชนิดของคำ การตรวจจับเอนทิตี หรือการแปล ส่งผลให้เป็นโมเดลสามารถประมวลผลภาษาธรรมชาติได้อย่างแม่นยำในระดับหนึ่ง และเป็นโมเดลที่ใช้งานสะดวก เพราะสามารถใช้โมเดลเดียวกันกับหลาย ๆ งานได้ เพียงแค่ปรับคำสั่งที่ป้อนให้กับโมเดล ทว่าโมเดลภาษาขนาดใหญ่มักจะไม่แม่นยำเท่าโมเดลแบบเฉพาะเจาะจง เนื่องจากโมเดลภาษาขนาดใหญ่ถูกพัฒนาขึ้นให้ใช้ได้โดยกว้าง ผู้พัฒนาไม่ได้ตั้งใจให้โมเดลแม่นสำหรับงานการประมวลผลภาษาธรรมชาติอย่างใดอย่างหนึ่งเท่านั้น 
โมเดลภาษาขนาดใหญ่ได้เปรียบโมเดลแบบเฉพาะเจาะจงตรงที่โมเดลภาษาขนาดใหญ่ไม่จำเป็นต้องใช้ชุดข้อมูลในการฝึกฝนงานใดงานหนึ่งโดยตรง ดังนั้นหากมีงานที่การสร้างชุดข้อมูลอาจจะใช้เวลานานเกินไป ตัวอย่างข้อมูลเข้าถึงได้ยาก หรือไม่สามารถหาผู้เชี่ยวชาญได้เพียงพอต่อการสร้างชุดข้อมูลที่มีขนาดใหญ่พอที่จะนำไปฝึกโมเดลได้ เช่น หากต้องการสร้างชุดข้อมูลการวิเคราะห์อารมณ์ของผู้ป่วยโรคซึมเศร้าจากบทสนทนากับจิตแพทย์ ข้อมูลมักจะเป็นความลับของผู้ป่วย เข้าถึงได้ยาก รวมถึงอาจจะหาเวลาจากจิตแพทย์ที่ยินดีมาช่วยกำกับข้อมูลได้ยาก ทำให้การสร้างชุดข้อมูลชุดนี้ต้องใช้ทรัพยากรมาก และอาจจะใช้เวลานานในการขออนุญาตเข้าถึงข้อมูลของผู้ป่วย ในกรณีนี้โมเดลภาษาขนาดใหญ่นับว่าเป็นทางเลือกที่ดีที่สุด เพราะความสามารถภาษาของโมเดลภาษาขนาดใหญ่สามารถปรับใช้ได้กับข้อมูลหลากหลายบริบท ทั้งนี้การนำไปใช้ต้องคำนึงถึงความแม่นยำที่สามารถคาดหวังได้ และจึงอาจจะต้องมีการสร้างชุดข้อมูลปริมาณหนึ่งเพื่อใช้ประเมินความแม่นยำของโมเดล ก่อนที่จะนำโมเดลไปใช้จริง ในบทต่อไปจะพูดถึงวิธีการปรับใช้โมเดลภาษาขนาดใหญ่กับงานการประมวลผลภาษาธรรมชาติที่อาจจะไม่มีโมเดลแบบเฉพาะเจาะจง


## การติดป้ายกำกับชนิดของคำ

ป้ายกำกับชนิดของคำ (part-of-speech tag) ในเชิงภาษาศาสตร์ถูกสร้างขึ้นมาเป็นส่วนหนึ่งของกฎไวยากรณ์ของภาษาที่กำหนดว่าคำชนิดใดสามารถมีความสัมพันธ์กับคำชนิดใด ในลักษณะไหนได้บ้าง  นอกจากนั้นแล้วยังมีบทบาทสำคัญในการทำความเข้าใจความหมายและโครงสร้างของประโยค
เนื่องจากช่วยให้เรารู้ว่าแต่ละคำในประโยคทำหน้าที่อะไร  เช่น คำนาม (noun) มักจะทำหน้าที่เป็นประธานของประโยค หรือเป็นส่วนเติมเต็มของกริยา หรือ คุณศัพท์ (adjective) มักจะทำหน้าที่เป็นตัวขยายความหมายของคำนาม เป็นต้น 

```{margin} คำศัพท์
ชนิดของคำ (part-of-speech) ชนิดของคำแบ่งตามลักษณะไวยากรณ์
```

ในการประมวลผลภาษาธรรมชาติ ชนิดของคำมักจะช่วยในการสกัดข้อมูล (information extraction) จากข้อมูลที่เป็นข้อความ เช่น ถ้าหากเราต้องการสกัดเอาชื่อของบุคคลจากข้อความ เราอาจจะเลือกเอาเฉพาะคำที่เป็นคำนามมาพิจารณาเท่านั้น หรือถ้าหากเราต้องการสกัดคำหลักที่บ่งบอกถึงความหมายใจความสำคัญของคลังเอกสาร เราอาจจะเลือกเฉพาะไบแกรมที่มีคำนามและคำกริยามาพิจารณาเท่านั้น 


```{margin} คำศัพท์
การสกัดข้อมูล (information extraction) การสกัดข้อมูลเฉพาะที่ต้องการจากข้อมูลที่ไม่มีโครงสร้าง เช่น ข้อความ ข่าว
```

### การจำแนกชนิดของคำ

ทฤษฎีการจำแนกชนิดของคำมีอยู่หลากหลายทฤษฎี แต่ละทฤษฎีต่างก็จำแนกชนิดของคำในจำนวนแตกต่างกัน ทั้งนี้นักเรียนไทยมักจะคุ้นเคยกับการจำแนกชนิดของคำตามหลักภาษาไทยของพระยาอุปกิตศิลปสารซึ่งจำแนกชนิดของคำเป็น 7 ชนิด ได้แก่ คำนาม คำสรรพนาม คำวิเศษณ์ คำกริยา คำบุพบท คำสันธาน และคำอุทาน อย่างไรก็ตามในการประมวลผลภาษาธรรมชาติ เรามักใช้ทฤษฎีของ ชุดป้ายกำกับชนิดของคำสากล (Universal Part-of-speech Tagset) {cite}`petrov-etal-2012-universal`      ทฤษฎีนี้กำหนดชนิดของคำออกเป็น 17 ชนิด ซึ่งสามารถจัดออกเป็น 3 กลุ่มใหญ่ ได้แก่ 

```{margin} คำศัพท์
ชุดป้ายกำกับชนิดของคำสากล (Universal Part-of-speech Tagset) ชุดป้ายกำกับที่ถูกพัฒนาขึ้นเพื่อรองรับการกำกับชนิดคำต่างภาษาให้เป็นไปในรูปแบบเดียวกัน
```


1. กลุ่มคำเปิด (open class word) คือ กลุ่มคำที่สามารถเพิ่มคำใหม่เข้าไปได้เรื่อย ๆ คำเปิดมักมีความหมายที่ชัดเจนและสามารถปรากฏได้โดยไม่ต้องพึ่งพาคำอื่น เช่น การกระทำ คน สัตว์ สิ่งของ สถานที่ กลุ่มคำเปิดประกอบด้วยคำ 6 ชนิด ได้แก่
    - คำวิเศษณ์ (adverb)
    - คำคุณศัพท์ (adjective)
    - คำอุทาน (interjection)
    - คำนาม (noun)
    - คำกริยา (verb)
    - วิสามานยนาม หรือคำนามชื่อเฉพาะ (proper noun)
2. กลุ่มคำปิด (closed class word) คือ กลุ่มคำที่มีจำนวนจำกัดและไม่ค่อยมีการเพิ่มคำใหม่ หน้าที่หลักของคำปิดคือการเชื่อมโยงหรือจัดระเบียบความสัมพันธ์ระหว่างคำในประโยค คำปิดมักมีความหมายที่น้อยลงหรือเป็นนามธรรม และปรากฏได้ก็ต่อเมื่อมีความสัมพันธ์กับคำอื่นเพื่อสร้างประโยคที่ถูกหลักไวยากรณ์ กลุ่มคำปิดประกอบด้วยคำ 8 ชนิด ได้แก่  
    - คำบุพบท (adposition)
    - คำช่วยกริยา (auxiliary)
    - คำนำหน้านาม (determiner)
    - ตัวเลข (numeral)
    - คำอนุภาค (particle)
    - คำสรรพนาม (pronoun)
    - คำสันธานเชื่อมความ (coordinating conjunction)
    - คำสันธานซ้อนความ (subordinating conjunction)
3. กลุ่มคำชนิดอื่น ๆ ประกอบด้วยคำ 3 ชนิด ได้แก่ 
    - เครื่องหมายวรรคตอน (punctuation)
    - สัญลักษณ์ (symbol)
    - คำอื่น ๆ ที่ไม่เข้าพวกกับคำชนิดที่กล่าวมาแล้ว

ชุดป้ายกำกับชนิดของคำสากลเป็นระบบการจำแนกชนิดของคำที่มีมาตรฐานเดียวกันทุกภาษา เพื่อให้การทำงานของโมเดลภาษาต่าง ๆ สามารถทำงานร่วมกันได้โดยสะดวกต่อการใช้โมเดลที่มีการใช้การถ่ายโอนข้ามภาษา (cross-lingual transfer) การใช้ชุดป้ายกำกับชนิดของคำสากล จึงเป็นเครื่องมือที่สำคัญและได้รับความนิยมในการพัฒนาระบบประมวลผลภาษาธรรมชาติที่มีประสิทธิภาพ

## การรู้จำเอนทิตี

เอนทิตี (entity) ในเชิงทฤษฎี คือ สิ่งที่มีตัวตนจริงและอิสระไม่เกี่ยวพันกับสิ่งอื่น ๆ ซึ่งเรามักจะหมายถึงบุคคล องค์กร บริษัท หรือสถานที่ ดังนั้นเอนทิตีที่มีชื่อเรียก (named entity) แปลตรงตัวแล้วจะหมายถึงบุคคล องค์กร บริษัท หรือสถานที่ที่มีการตั้งชื่อเรียก 
แต่ในบริบทของการประมวลผลภาษาธรรมชาติ การรู้จำเอนทิตี (Named-Entity Recognition หรือ NER) หมายความกว้างกว่านั้น การรู้จำเอนทิตี หมายถึงการตรวจจับชื่อของบุคคล ชื่อขององค์กร ชื่อของสถานที่ ชื่อของเหตุการณ์ วันเดือนปี หรือชื่อของสิ่งของอื่น ๆ ที่มีความสำคัญออกมาจากข้อความ และในทางปฏิบัติแล้วการรู้จำเอนทิตีไม่ได้จำกัดเพียงแต่เอนทิตีอย่างเดียว แต่หมายความรวมถึงการตรวจจับส่วนอื่นของข้อความที่มีความสำคัญ​และอาจจะไม่ได้มีตัวตนอิสระตามความหมายของเอนทิตี เช่น

```{margin} คำศัพท์
เอนทิตี (entity) (ทั่วไป) สิ่งหรือแนวคิดที่มีตัวตนอยู่
```

- เวลาและวันที่ เช่น "เมื่อวาน" หรือ "สัปดาห์หน้า"
- จำนวนเงิน เช่น "500 บาท" หรือ "หนึ่งพันเหรียญสหรัฐ"
- จำนวนและขนาด เช่น "3 กิโลเมตร" หรือ "5 ลิตร"
- คำนำหน้าชื่อ และคำศัพท์ทั่วไปที่เป็นคำบ่งบอกสถานที่ เช่น "เด็กชาย" "นางสาว" หรือ "โรงเรียน" ซึ่งไม่ได้บ่งบอกถึงชื่อเฉพาะของบุคคลหรือสถานที่

การรู้จำเอนทิตีมีประโยชน์อย่างมากในการประยุกต์ใช้การประมวลผลภาษาธรรมชาติ โดยสามารถยกตัวอย่างได้ดังนี้ 

- ในการวิเคราะห์ความเห็นของลูกค้าเกี่ยวกับผลิตภัณฑ์ในโซเชียลมีเดีย บริษัทสามารถใช้การรู้จำเอนทิตีเพื่อระบุชื่อผลิตภัณฑ์ ชื่อบริษัท หรือบุคคลที่เกี่ยวข้อง จากนั้นสามารถวิเคราะห์ความเห็นในเชิงบวกหรือลบที่มีต่อเอนทิตีเหล่านี้ได้อย่างมีประสิทธิภาพ ซึ่งช่วยให้บริษัทสามารถเข้าใจความคิดเห็นของลูกค้าเกี่ยวกับผลิตภัณฑ์หรือบริการได้อย่างละเอียด และสามารถปรับปรุงผลิตภัณฑ์และการบริการตามความคิดเห็นที่ได้รับได้อย่างตรงจุด 
- ในระบบตอบคำถามอัตโนมัติ เช่น ระบบบริการหลังการขายออนไลน์  การรู้จำเอนทิตีสามารถระบุข้อมูลสำคัญจากคำถามของผู้ใช้ เช่น ชื่อผลิตภัณฑ์ หรือปัญหาที่เกิดขึ้น ทำให้ระบบสามารถให้คำตอบที่แม่นยำและรวดเร็วแก่ผู้ใช้ ลดเวลาที่ต้องใช้ในการค้นหาข้อมูลและเพิ่มประสิทธิภาพในการให้บริการลูกค้า
- ในด้านการสกัดข้อมูลทางการแพทย์ การรู้จำเอนทิตีมีบทบาทสำคัญในการวิเคราะห์เวชระเบียนหรือเอกสารทางการแพทย์ โดยสามารถระบุชื่อโรค ชื่อยา ชื่อแพทย์ หรือวันนัดหมายได้อย่างชัดเจน ซึ่งช่วยในการจัดการข้อมูลทางการแพทย์อย่างเป็นระบบ สามารถสกัดข้อมูลสำคัญเพื่อใช้ในการวินิจฉัยโรค หรือการวิจัยทางการแพทย์ได้อย่างรวดเร็วและแม่นยำ 
- ในการวิเคราะห์ข่าวสารจากแหล่งต่าง ๆ การรู้จำเอนทิตีสามารถระบุชื่อบุคคลสำคัญ เหตุการณ์สำคัญ สถานที่ หรือชื่อองค์กรที่ปรากฏในข่าว ทำให้สำนักข่าวหรือองค์กรที่เกี่ยวข้องสามารถติดตามและวิเคราะห์ข้อมูลข่าวสารได้อย่างมีประสิทธิภาพ สามารถสรุปแนวโน้มข่าวสาร หรือทำการวิจัยเชิงลึกเกี่ยวกับประเด็นที่สำคัญได้ 


การรู้จำเอนทิตีมักจะใช้เทคนิคการประมวลผลภาษาธรรมชาติเช่นเดียวกับการจำแนกชนิดของคำ โดยใช้โมเดลการเรียนรู้ของเครื่องที่เรียนรู้จากชุดข้อมูลที่มีเอนทิตีที่ถูกติดป้ายกำกับไว้ และใช้เอนทิตีที่ติดป้ายกำกับเหล่านั้นในการสร้างโมเดลที่สามารถรู้จำเอนทิตีใหม่ได้

## ไลบรารี spacy สำหรับการติดป้ายชนิดของคำ และการรู้จำเอนทิตี

การติดป้ายชนิดของคำ (part-of-speech tagging) และการรู้จำเอนทิตีอัตโนมัติจำเป็นต้องใช้โมเดลแบบการเรียนรู้ของเครื่องในการแยก เพราะฉะนั้นไลบรารีที่ใช้จะต้องมีโมเดลเหล่านี้เตรียมไว้ให้พร้อมใช้ ไลบรารี spacy เป็นไลบรารีที่รองรับการประมวลผลภาษาธรรมชาติหลากหลายภาษา รวมถึงมีโมเดลการประมวลผลภาษาธรรมชาติหลากหลายขนาดให้เลือกใช้ตามความเหมาะสม และตามข้อจำกัดในการนำโมเดลไปใช้จริง  {cite}`spacy` ปัจจุบัน (2024) spacy รองรับการติดป้ายชนิดของคำสำหรับภาษาหลายภาษา เช่น ภาษาอังกฤษ ภาษาสเปน ภาษาฝรั่งเศส ภาษาจีน ภาษาญี่ปุ่น ภาษาเกาหลี แต่ว่า spacy ยังไม่มีโมเดลในการประมวลผลภาษาไทย

ขั้นแรกต้องหาชื่อชุดโมเดลที่เราต้องการใช้ รายชื่อชุดโมเดลอยู่บนเว็บไซต์ของ spacy ซึ่งมีเอกสารประกอบการใช้งานที่เป็นปัจจุบันอยู่ โมเดลเหล่านี้ประกอบไปด้วยโมเดลย่อยหลาย ๆ โมเดลด้วยกัน เราสามารถตรวจสอบดูบนเว็บไซต์ได้ว่าโมเดลแต่ละชุดประกอบด้วยโมเดลอะไรบ้าง  ตัวอย่างเช่น สำหรับภาษาจีน มีชุดโมเดลที่มีโมเดลการแยกชนิดของคำได้อยู่สามโมเดลได้แก่ `zh_core_web_sm` (ขนาดเล็ก) `zh_core_web_md` (ขนาดกลาง) และ  `zh_core_web_lg` (ขนาดใหญ่) ทั้งสามชุดในเวอร์ชันปี 2024 ประกอบไปด้วย 5 โมเดลย่อย ได้แก่ โมเดลการตัดคำ โมเดลการตัดประโยค โมเดลการวิเคราะห์โครงสร้างประโยค โมเดลการติดป้ายชนิดของคำ โมเดลการรู้จำเอนทิตี  

หากเรายังไม่ได้ติดตั้งไลบรารี spacy เราสามารถติดตั้งโดยใช้คำสั่ง `pip install spacy` หลังจากนั้นเราสามารถดาวน์โหลดและติดตั้งโมเดลที่ต้องการได้โดยใช้คำสั่ง `python -m spacy download` ตามด้วยชื่อโมเดลที่ต้องการดาวน์โหลดและติดตั้ง เช่น

```bash
python -m spacy download zh_core_web_md
```

วิธีการใช้ spacy จะต่างจากการใช้ pythainlp ตรงที่ว่าเราจะต้องสร้างอ็อบเจกต์ที่ทำหน้าที่เป็นตัวประมวลผลที่โหลดโมเดลที่ต้องการมาให้พร้อม เพื่อที่จะได้ไม่ต้องโหลดโมเดลซ้ำ ๆ เพราะการโหลดโมเดลขนาดใหญ่ซ้ำ ๆ จะทำให้โปรแกรมทำงานช้า ไม่เหมาะกับการประมวลผลข้อมูลขนาดใหญ่ที่เรามักจะต้องวนซ้ำการประมวลผลบนข้อมูลทีละชิ้น ตัวอย่างการใช้งาน spacy ในการจำแนกชนิดของคำแสดงในโค้ดต่อไปนี้

```python 
import spacy
model_name = 'zh_core_web_md'
nlp_processor = spacy.load(model_name)
text  = '张伟和李娜在漂亮的上海吃饭'
doc = nlp_processor(text)
for token in doc:
    print(token.text, token.pos_)
```

ผลลัพธ์ที่ได้จะแสดงชนิดของคำที่แยกออกมาจากข้อความ ตามที่กำหนดไว้ในชุดป้ายกำกับชนิดของคำสากลดังนี้

```
张伟 PROPN
和 CCONJ
李娜 PROPN
在 ADP
漂亮 VERB
的 PART
上海 PROPN
吃饭 VERB
```

 ในตัวอย่างนี้เราโหลดโมเดล `zh_core_web_md` ซึ่งเป็นโมเดลขนาดกลางสำหรับภาษาจีน โดยการเรียกคำสั่ง `spacy.load` เพื่อสร้างตัวประมวลผลข้อความและเก็บไว้ในตัวแปรชื่อ `nlp_processor` ซึ่งสามารถใช้ได้คล้ายกับว่าเป็นฟังก์ชัน เราจึงสามารถเรียก `nlp_processor(text)` ได้เลยโดยไม่ต้องใช้ชื่อเมท็อดอื่น ผลที่ได้คือเป็น `Doc` อ็อบเจกต์ที่เราสามารถวนลูปเพื่อดึง `Token` อ็อบเจกต์ออกมาและดึงชนิดของคำออกมาได้โดยใช้ `token.pos_` ในการเข้าถึงชนิดของคำ


 ข้อสังเกตที่สำคัญอีกประการหนึ่งของการใช้ spacy คือ ไลบรารีนี้ประมวลข้อความเป็นแบบการทำงานแบบสายท่อ (pipeline) กล่าวคือสตริงที่รับเข้ามาจะถูกประมวลหลาย ๆ ขั้นในครั้งเดียว เช่น การประมวลภาษาจีนจะเริ่มด้วยการตัดคำ ต่อด้วยการแยกชนิดของคำ การวิเคราะห์โครงสร้างประโยค และจบด้วยการรู้จำเอนทิตี จากนั้นผู้ใช้สามารถเข้าถึงผลการวิเคราะห์ทั้งหมดนี้ผ่าน `Doc` อ็อบเจกต์ที่ได้รับกลับมาจากการเรียก `nlp_processor(text)` โดยไม่ต้องเรียกแต่ละขั้นตอนเอง  เพราะฉะนั้นเราสามารถเข้าถึงรายชื่อเอนทิตีได้เลย โดยไม่ต้องประมวลผลข้อความอีกครั้ง

 ```python
for ent in doc.ents:
    print(ent.text, ent.label_)
```

ผลลัพท์ที่ได้จะแสดงสตริงที่อ้างถึงเอนทิตี และชนิดของเอนทิตี ดังนี้

```
张伟 PERSON
李娜 PERSON
上海 GPE
```
จากข้อความตัวอย่างที่ป้อนเข้าไป ตัวประมวลผลรู้จำชื่อเอนทิตีที่เป็นบุคคลทั้งหมดสองชื่อ ได้แก่  张伟 (จางเหว่ย) และ 李娜 (หลีน่า) และเอนทิตีแบบภูมิรัฐศาสตร์ (GPE ซึ่งย่อมาจาก geopolitical entity) หนึ่งชื่อ 上海 (เมืองเซี่ยงไฮ้)


## การวิเคราะห์อารมณ์ความรู้สึก 

การวิเคราะห์อารมณ์ความรู้สึก (sentiment analysis) คือ กระบวนการในการตรวจจับและจำแนกอารมณ์หรือความรู้สึกที่แสดงออกมาในข้อความ โดยใช้เทคนิคทางด้านการประมวลผลภาษาธรรมชาติ และการเรียนรู้ของเครื่อง การวิเคราะห์อารมณ์ความรู้สึกมักถูกใช้เพื่อประเมินความคิดเห็นของผู้ที่ใช้งานระบบ หรือผู้บริโภคที่มีต่อผลิตภัณฑ์ บริการ รวมถึงการประเมินความคิดเห็นของบุคคลทั่วไปที่มีต่อเหตุการณ์ต่าง ๆ โดยสามารถจำแนกออกเป็นอารมณ์เชิงบวก เชิงลบ หรือเป็นกลาง การวิเคราะห์อารมณ์ความรู้สึกนับว่าเป็นการประยุกต์ใช้การประมวลผลภาษาธรรมชาติในโลกของวิทยาการข้อมูลที่เห็นเด่นชัด และเป็นที่นิยมที่สุดชนิดหนึ่งในปัจจุบัน ตัวอย่างของการวิเคราะห์อารมณ์ความรู้สึกมีอยู่หลากหลายแบบ เช่น

```{margin} คำศัพท์
การวิเคราะห์อารมณ์ความรู้สึก (sentiment analysis) การวิเคราะห์และจัดกลุ่มอารมณ์ความรู้สึกที่ปรากฏในข้อความหรือข้อมูล
```

- การวิเคราะห์ความคิดเห็นของลูกค้าในโซเชียลมีเดีย: สมมติว่าเรามีข้อความจากทวิตเตอร์เกี่ยวกับผลิตภัณฑ์ใหม่ของบริษัท เราสามารถแยกแยะความคิดเห็นเชิงบวก ลบ หรือกลาง และนับจำนวนความคิดเห็นแต่ละประเภทได้ และแยกวิเคราะห์ได้ บริษัทสามารถใช้ข้อมูลนี้เพื่อเข้าใจความคิดเห็นของลูกค้าต่อผลิตภัณฑ์ใหม่ และนำไปปรับปรุงผลิตภัณฑ์หรือกลยุทธ์การตลาดได้

- การประเมินความพึงพอใจของลูกค้าในบทวิจารณ์ออนไลน์:
การวิเคราะห์อารมณ์ความรู้สึกสามารถแยกแยะได้ว่าอาจจะมีความรู้สึกเชิงบวกต่อผลิตภัณฑ์ แต่มีความรู้สึกเชิงลบต่อการบริการ ร้านอาหารสามารถใช้ข้อมูลนี้เพื่อปรับปรุงการบริการและรักษาคุณภาพอาหารให้ดีอยู่เสมอ

- การวิเคราะห์อารมณ์ความรู้สึกที่อยู่เอกสารทางการเงิน ทุก ๆ ปีบริษัทที่มีการซื้อขายหุ้นอยู่ในตลาดหลักทรัพย์ จะต้องเผยแพร่เอกสารรายงานผลประกอบการที่จะต้องพูดถึงบริษัทในหลากหลายด้าน เช่น เรื่องผลกำไรขาดทุน ความเสี่ยงในการประกอบธุรกิจ การควบรวมกับบริษัทอื่น การพัฒนากำลังคน หรือการเปลี่ยนแปลงในโครงสร้างบริษัท นักวิเคราะห์สามารถวิเคราะห์อารมณ์ความรู้สึกในเอกสารรายงานผลประกอบการ เพื่อให้ได้ดัชนีในการชี้วัดว่าผู้เขียนมีความคิดเห็นในด้านบวก ลบ หรือกลางในด้านใดบ้างในปีที่ผ่านมา ซึ่งมีการศึกษามาว่าผลการวิเคราะห์ในลักษณะดังกล่าวมีความสัมพันธ์ร่วมกันกับผลตอบแทนเกินปกติ (abnormal return) ดังนั้นการวิเคราะห์อารมณ์ความรู้สึกจึงสามารถช่วยให้นักลงทุนหรือผู้บริหารบริษัทมีดัชนีชี้วัดเพิ่มอีกตัวหนึ่งที่ช่วยในการตัดสินใจลงทุนในสินทรัพย์ต่าง ๆ ได้อย่างมีประสิทธิภาพมากขึ้น 

- การติดตามความคิดเห็นเรื่องการเมืองบนสื่อออนไลน์: การศึกษาเรื่องวิเคราะห์อารมณ์ความรู้สึกที่อยู่บนสื่อออนไลน์ เช่น เว็บไซต์ reddit สามารถช่วยให้นักวิเคราะห์เข้าใจความคิดเห็นของประชาชนต่อเรื่องการเมือง เนื่องจากเหตุการณ์ทางการเมืองต่าง ๆ ส่งผลต่ออารมณ์ความรู้สึกที่ปรากฏอยู่ในสื่อเหล่านี้ {cite}`tachaiya2021raffman` การวิเคราะห์อารมณ์ความรู้สึกบนฟอรัมออนไลน์มีประโยชน์อย่างมากในการทำความเข้าใจและตอบสนองต่อความคิดเห็นของประชาชนได้อย่างมีประสิทธิภาพ โดยช่วยให้เราสามารถติดตามการเปลี่ยนแปลงของอารมณ์ความรู้สึกในช่วงเวลาต่าง ๆ เมื่อเกิดเหตุการณ์สำคัญ เช่น การถอดถอนประธานาธิบดี ซึ่งจะเป็นประโยชน์ในการระบุแนวโน้มทางการเมืองและสังคม นอกจากนี้ ยังสามารถช่วยในการวิเคราะห์แนวโน้มการแบ่งขั้วทางการเมืองหรือความรู้สึกที่มีต่อประเด็นสำคัญในสังคมอีกด้วย 

การวิเคราะห์อารมณ์ความรู้สึกในเชิงปฏิบัติ คือ การแยกประเภทของข้อความว่าเป็นข้อความที่แสดงความเห็นในด้านบวก ด้านลบ หรือเป็นกลาง เราสามารถคิดว่าตัววิเคราะห์อารมณ์ความรู้สึก (sentiment analyzer) เป็นฟังก์ชันที่รับสตริงข้อความ และคืนค่าเป็นป้ายกำกับ (label) ที่บ่งบอกว่าข้อความนั้นเป็นเชิงบวก ลบ หรือเป็นกลาง โดยใช้เทคนิคการเรียนรู้ของเครื่อง ซึ่งสามารถทำได้ด้วยการใช้ชุดข้อมูลที่มีป้ายกำกับความรู้สึกเช่นข้อความที่มีป้ายกำกับว่าเป็นเชิงบวก ลบ หรือเป็นกลาง ในการสร้างโมเดลการเรียนรู้ของเครื่อง โดยใช้ข้อมูลเหล่านี้ในการเรียนรู้ว่าคุณลักษณะของข้อความที่เป็นเชิงบวก ลบ หรือกลาง คืออะไร และใช้คุณลักษณะเหล่านั้นในการจำแนกประเภทของข้อความใหม่ที่ยังไม่เคยเห็นมาก่อน หนึ่งในชุดข้อมูลที่ได้รับความนิยมมากในการนำมาสร้างตัววิเคราะห์อารมณ์ความรู้สึก คือ Sentiment Treebank  ซึ่งนำมาแสดงให้ดูเป็นตัวอย่างดังนี้ โดยเลือกเฉพาะตัวอย่างที่มีคำว่า *excellent* ซึ่งแปลว่ายอดเยี่ยมอยู่อย่างน้อย 1 จุด {cite}`sentimenttreebank`

|  | ข้อความ | ป้ายกำกับ |
|---|---------|-------------------|
|(a) | Leguizamo and Jones are both excellent and the rest of the cast is uniformly superb. | บวก |
|(b) | Still, this flick is fun, and host to some truly excellent sequences. | บวก |
|(c) | A potentially good comic premise and excellent cast are terribly wasted. | ลบ |
|(d)| There's an excellent 90-minute film here; unfortunately, it runs for 170 | ลบ | 
|(e)| So brisk is Wang's pacing that none of the excellent cast are given air to breathe. | เป็นกลาง |

ข้อสังเกตที่สำคัญอย่างหนึ่งจากตัวอย่างข้างต้น คือ ถึงแม้ว่าทุกข้อความจะมีคำว่า *excellent* อยู่  แต่ก็ไม่ได้หมายความว่าอารมณ์ความรู้สึกที่ส่งผ่านข้อความนั้นจะเป็นบวกเสมอไป ยังมีปัจจัยทางบริบทที่มีผลต่อการตีความโดยรวม เช่น บริบททางโครงสร้างประโยคในตัวอย่าง (c) นำเอาคำว่า *excellent* มาใช้เป็นส่วนขยายของนามแต่ว่าไม่ได้นำมาใช้เป็นส่วนหนึ่งของกริยาหลัก ซึ่งมีผลต่อความหมายโดยรวมของประโยคมากกว่า ทำให้เห็นว่าเราต้องคำนึงปัจจัยทางภาษาศาสตร์อื่น ๆ อีกมาก นอกเหนือจากความหมายของคำ จึงจะสามารถวิเคราะห์อารมณ์ความรู้สึกได้ถูกต้อง

## แพลตฟอร์ม huggingface และไลบรารี transformers สำหรับการวิเคราะห์อารมณ์ความรู้สึก 

Huggingface เป็นแพลตฟอร์ม (ระบบที่ทุกคนเข้าใช้ร่วมกัน แทนที่จะมีคนกลุ่มเดียวที่เข้าใช้ได้) ที่มุ่งเน้นการพัฒนาและแบ่งปันโมเดลการประมวลผลภาษาธรรมชาติที่มีความสามารถหลากหลาย กล่าวคือนักพัฒนาสามารถนำโมเดลที่ตนสร้างขึ้นมาฝากไว้บนแพลตฟอร์มนี้ เพื่อให้ผู้ใช้คนอื่นเข้ามาโหลดเพื่อนำไปใช้ได้อย่างสะดวก ซึ่งหน้าที่ของแพลตฟอร์มคืออำนวยความสะดวกให้ทั้งสองฝ่ายโดยไม่ต้องพัฒนาโมเดลทั้งหมดเอง  

```{margin} คำศัพท์
แพลตฟอร์ม (platform) กรอบการทำงานที่ให้บริการระบบเพื่อให้ผู้ใช้งานสามารถสื่อสารและทำงานร่วมกันได้อย่างมีประสิทธิภาพ
```

แพลตฟอร์ม huggingface อยู่บนเว็บไซต์ https://huggingface.co  และมาพร้อมกับไลบรารีที่เรียกว่า transformers ซึ่งเป็นไลบรารีไพทอนที่ช่วยโหลดโมเดลมาจากแพลตฟอร์ม เพื่อนำไปพัฒนาเป็นระบบอื่น ๆ ในภาษาไพทอนได้อย่างสะดวก huggingface รวมโมเดลที่ถูกพัฒนามาแล้วหลากหลายรุ่นที่สามารถนำไปใช้ในงานต่าง ๆ ได้ทันที ถ้าหากเราค้นหาโมเดล sentiment analysis บนแพลตฟอร์ม ก็จะพบว่ามีโมเดลให้เลือกกว่า 100 โมเดล เราจำเป็นต้องดูว่าโมเดลใดรองรับภาษาที่เราต้องการวิเคราะห์ รวมถึงตรวจสอบอีกว่าความแม่นยำที่คาดหวังได้คือเท่าไร ผู้ที่นำโมเดลมาฝากในแพลตฟอร์มนี้มักจะมีเขียนไว้อย่างละเอียดถึงกระบวนการพัฒนาโมเดล เหมาะกับข้อมูลประเภทใด (เช่น ทวีต รีวิว หรือข่าวการเงิน) และความแม่นยำตามที่ได้ประเมินประสิทธิภาพไว้ 

เริ่มต้นจากการติดตั้งไลบรารีด้วยคำสั่ง `pip install transformers` วิธีการใช้ไลบรารีนี้แบบพื้นฐานที่สุด ทำได้โดยการใช้ฟังก์ชัน `pipeline` ในการโหลดโมเดลและปรับการใช้งาน โดยที่เราจะต้องทราบชื่อโมเดลที่ต้องการใช้ 
โมเดลที่แนะนำในปัจจุบัน (มิถุนายน 2567) สำหรับการวิเคราะห์อารมณ์ความรู้สึกภาษาไทยคือ `poom-sci/WangchanBERTa-finetuned-sentiment` ซึ่งเรียนรู้การวิเคราะห์มาจากชุดข้อมูลที่เป็นรีวิว รวมกันกับข้อมูลที่เป็นทวีต ทำให้เหมาะกับการวิเคราะห์เบื้องต้น ไม่ได้เฉพาะเจาะจงกับข้อมูลแหล่งใดแหล่งหนึ่ง 

```python
# Use a pipeline as a high-level helper
from transformers import pipeline
pipe = pipeline("text-classification", model="poom-sci/WangchanBERTa-finetuned-sentiment")
pipe(["เนื้อครีมลื่นไม่เหนียว", "กระปุกสีขาว ฝาเกลียว", "เปิดออกมาแล้วกลิ่นค่อนข้างแรง"])
```

ผลลัพธ์ที่ได้ออกมาคือ ลิสต์ของดิกชันนารีที่ประกอบด้วยป้ายกำกับ (label) ที่มีค่าความน่าจะเป็นสูงสุดคือป้ายกำกับที่ และค่าความน่าจะเป็น (score) ที่ได้ ดังนี้

```python
[{'label': 'pos', 'score': 0.6775356531143188},
 {'label': 'neu', 'score': 0.9761248230934143},
 {'label': 'neg', 'score': 0.7795576453208923}]
```

ภาษาไทยมีโมเดลอีกตัวหนึ่งซึ่งสร้างขึ้นมาเพื่อข้อมูลที่มาจากเอกสารทางการเงิน เช่น รายงานผลประกอบการ หรือข่าวทางการเงิน โมเดลตัวนี้ชื่อว่า `nlp-chula/augment-sentiment-finnlp-th` บนแพลต์ฟอร์ม huggingface เราสามารถนำมาใช้งานได้ด้วยคำสั่งเดียวกันเพียงแต่เปลี่ยนชื่อโมเดลที่ใช้ 

```python
pipe_finance_news = pipeline("text-classification", model="nlp-chula/augment-sentiment-finnlp-th")
pipe_finance_news([
    "บริษัทยังคงดำเนินนโยบายการขยายเครือข่ายสาขาอย่างต่อเนื่อง", 
     "ในช่วงครึ่งปีหลัง บริษัทเน้น เรื่องการเร่งรัดติดตามทวงถามหนี้เพิ่มมากขึ้น",
    "การแพร่ระบาดของไวรัสโควิด-19 ระลอกใหม่เป็นปัจจัย สาคัญที่กดดันต่อการบริโภคภาคเอกชน และการท่องเที่ยว",
    "ธนาคารมีสินทรัพย์สภาพคล่องประมาณ 47,272 ล้านบาท ลดลงจานวน 2,424 ล้านบาท ",
    "ธุรกิจกองทุนสํารองเลี้ยงชีพของ บลจ. ทิสโก้ยังคงเติบโตกว่าร้อยละ 11.8 เทียบกับ อุตสําหกรรมที่เติบโตลดลงเหลือเพียงร้อยละ 2.1 จากปี 2562"])
```

ผลลัพธ์ที่ได้ออกมาคือ 

```python
[{'label': 'Neutral', 'score': 0.6528419256210327},
 {'label': 'Neutral', 'score': 0.937697172164917},
 {'label': 'Negative', 'score': 0.9270865321159363},
 {'label': 'Negative', 'score': 0.8279978036880493},
 {'label': 'Positive', 'score': 0.5197721719741821}]
 ```

ข้อควรระวังอย่างหนึ่งของการวิเคราะห์อารมณ์ความรู้สึก คือการเลือกใช้โมเดลให้เหมาะสมกับขอบเขตเนื้อหา (domain) ของข้อมูลที่เราต้องการวิเคราะห์  ถ้าหากชุดข้อมูลที่ใช้พัฒนาโมเดลมีขอบเขตเนื้อหาที่ไม่ตรงกับ ขอบเขตเนื้อหาของข้อมูลที่เราต้องการวิเคราะห์ อาจจะทำให้ความแม่นยำลดลง หรือผลลัพธ์ที่ได้ไม่เป็นที่พอใจ สมมติว่าโมเดลการวิเคราะห์อารมณ์ความรู้สึก (sentiment analysis) ถูกพัฒนามาด้วยข้อมูลรีวิวภาพยนตร์ ซึ่งข้อมูลนี้มีลักษณะเฉพาะ เช่น คำศัพท์และรูปแบบการใช้ภาษาที่เกี่ยวข้องกับการรีวิวภาพยนตร์ หากเรานำโมเดลนี้ไปใช้วิเคราะห์ความคิดเห็นเกี่ยวกับการเงินการลงทุน  โมเดลก็จะมีความแม่นยำลดลง เพราะลักษณะทางภาษาหรือขอบเขตเนื้อหาของข้อมูลรีวิวภาพยนตร์อาจมีความแตกต่างจากข้อมูลความเห็นเกี่ยวกับการเงินการลงทุน  เรียกอีกอย่างว่า ปัญหาการกระจายตัวแบบออกนอกขอบเขต (out-of-distribution) ซึ่งเป็นปัญหาที่สำคัญในการใช้โมเดลการเรียนรู้ของเครื่องในงานปัญญาประดิษฐ์ {cite}`ood2023`

ในตัวอย่างการใช้ huggingface ข้างต้น เราได้ลองใช้โมเดลภาษาไทยสองตัวที่ถูกพัฒนาขึ้นมาจากชุดข้อมูลคนละขอบเขตกัน ตัวแรกสร้างขึ้นเพื่อใช้วิเคราะห์รีวิวและทวีต และโมเดลตัวที่สองสร้างขึ้นเพื่อใช้วิเคราะห์ข้อความที่ขอบเขตเนื้อหาคือการเงิน และผลประกอบการของบริษัท 
หากเราใช้โมเดลตัวแรกมาวิเคราะห์ข้อความที่พูดถึงเรื่องการเงิน จะได้ผลดังนี้

```python
[{'label': 'neu', 'score': 0.9173294305801392},
 {'label': 'neu', 'score': 0.9335467219352722},
 {'label': 'neu', 'score': 0.8750076293945312}, # ที่ถูกต้องคือ 'neg'
 {'label': 'neu', 'score': 0.9450038075447083}, # ที่ถูกต้องคือ 'neg'
 {'label': 'neu', 'score': 0.9596440196037292}] # ที่ถูกต้องคือ 'pos'
```

หากเราใช้โมเดลตัวที่สองมาวิเคราะห์ที่เป็นรีวิวจะได้ผลดังนี้

```python
[{'label': 'Neutral', 'score': 0.6709030270576477}, # ที่ถูกต้องคือ 'Positive'
 {'label': 'Positive', 'score': 0.5924004316329956},# ที่ถูกต้องคือ 'Neutral'
 {'label': 'Neutral', 'score': 0.8645471930503845}]# ที่ถูกต้องคือ 'Negative'
```

เมื่อโมเดลทั้งสองตัวถูกนำไปใช้กับข้อมูลที่ไม่ตรงกับขอบเขตเนื้อหาที่ใช้พัฒนา ความแม่นยำจะลดลง และผลลัพธ์ที่ได้ไม่เป็นที่พอใจ ดังนั้นการเลือกใช้โมเดลที่เหมาะสมกับข้อมูลที่ต้องการวิเคราะห์เป็นสิ่งสำคัญ โดยทั่วไปแล้วโมเดลที่ได้รับความนิยมบนแพลตฟอร์ม huggingface มักจะระบุว่าโมเดลถูกพัฒนาขึ้นมาด้วยชุดข้อมูลอะไร ที่มาของข้อความอยู่ในขอบเขตเนื้อหาอะไร เพื่อผู้ใช้จะได้นำไปประยุกต์ใช้ได้อย่างเหมาะสมกับข้อมูลที่ต้องการวิเคราะห์ 

## โมเดลหัวเรื่อง

โมเดลหัวเรื่อง (topic model) เป็นหนึ่งในโมเดลที่เป็นที่นิยมในการวิเคราะห์หาหัวเรื่องที่อยู่ในชุดเอกสาร จุดประสงค์ของโมเดลหัวเรื่องคือระบุและจัดกลุ่มข้อความที่มีเนื้อหาคล้ายคลึงกัน การทำงานของโมเดลหัวเรื่องนั้นคล้ายกับการวิเคราะห์ความถี่ของคำ (word frequency analysis) เพียงแต่ว่าเราไม่ต้องจัดคำที่ปรากฏบ่อยให้เป็นหมวดหมู่ด้วยตัวเอง โมเดลหัวเรื่องที่ได้รับความนิยมและใช้กันอย่างแพร่หลายคือ Latent Dirichlet Allocation (LDA) ซึ่งเป็นเทคนิคที่ช่วยในการหาหัวเรื่องทั้งหมดที่อยู่ในชุดเอกสารโดยการวิเคราะห์ความน่าจะเป็นของการเกิดร่วมกันของคำในเอกสาร นอกจากนั้นแล้วโมเดล LDA ยังช่วยในการระบุหัวเรื่องหลักในเอกสารแต่ละเอกสารอีกด้วย  ตัวอย่างเช่น หากเรามีชุดเอกสารที่เกี่ยวกับข่าวต่างประเทศ โมเดล LDA อาจช่วยในการระบุหัวเรื่องเช่น "การเมือง", "เศรษฐกิจ", และ "กีฬา" ทำให้เราสามารถจัดหมวดหมู่ข้อมูลและวิเคราะห์เชิงลึกได้ง่ายขึ้น นอกจากนี้ โมเดลหัวเรื่องยังสามารถนำไปใช้ในงานต่างๆ เช่น การจัดหมวดหมู่บทความในเว็บไซต์ข่าว การวิเคราะห์เอกสารทางวิชาการเพื่อค้นหาประเด็นวิจัยสำคัญ และการประมวลผลข้อความจากโซเชียลมีเดียเพื่อหาแนวโน้มและความสนใจของผู้ใช้งาน เป็นต้น

โมเดล LDA มีรายละเอียดทางคณิตศาสตร์ที่ซับซ้อน ซึ่งอยู่นอกเหนือขอบเขตของหนังสือเล่มนี้ {cite}`blei2003lda` อย่างไรก็ตาม เราสามารถเข้าใจหลักการทำงานของ LDA ได้อย่างง่าย ๆ โดยไม่ต้องลงลึกในรายละเอียดสมการของ LDA มากนัก LDA ทำงานโดยการหาคำที่เกิดขึ้นร่วมกันบ่อย ๆ ในเอกสารเดียวกัน สมมติว่าชุดเอกสารที่เราวิเคราะห์มีบทความข่าวทั้งหมด 10 ข่าว ซึ่งอาจจะประกอบไปด้วยข่าวการเมือง ข่าวเศรษฐกิจ และข่าวกีฬา แต่เราไม่ทราบมาก่อนเลยว่าข่าวทั้ง 10 ข่าวนี้จัดกลุ่มเป็นข่าวประเภทใดได้บ้าง แต่เราพอจะคาดเดาได้ว่าน่าจะมีสัก 3 ประเภทด้วยกัน 

ข้อสังเกตที่สำคัญที่ทำให้ LDA ทำงานได้อย่างมีประสิทธิภาพคือ เราสามารถจัดหัวเรื่องตามคำหลักได้ และคำหลักที่มาจากหัวเรื่องเดียวกันเหล่านี้มักพบเห็นพร้อมกันในเอกสารเดียวกัน เช่น  ข่าวการเมืองมักจะมีคำเฉพาะเจาะจงที่เกี่ยวข้องกับการเมือง เช่น *นายกรัฐมนตรี รัฐบาล รัฐสภา เลือกตั้ง ประกาศ ลงมติ* และข่าวการเมืองหนึ่งข่าวมักจะใช้คำเหล่านี้หลาย ๆ คำในข่าวเดียวกัน  ข่าวเศรษฐกิจมักจะมีคำเฉพาะเจาะจงที่เกี่ยวข้องกับเศรษฐกิจ เช่น *ธุรกิจ ชะลอตัว ลด การลงทุน ดอกเบี้ย อัตรา เศรษฐกิจ* ข่าวการเมืองก็อาจจะใช้คำหลักเหล่านี้บ้าง เพราะว่าข่าวการเมืองก็อาจจะพูดถึงเศรษฐกิจด้วย แต่ว่าไม่ใช่ข่าวการเมืองทุกข่าวจะพูดถึงเศรษฐกิจ เพราะเหตุนี้ผลนี้เอง โมเดล LDA จึงทำงานโดยการตรวจหาคำที่เกิดร่วมกันบ่อย ๆ ในเอกสารเดียวกัน และรวบรวมรายการของคำเหล่านี้รวมกันเป็นหัวเรื่อง เพราะฉะนั้น*หัวเรื่อง* ที่ได้จาก LDA คือรายการของคำและแต่ละคำมีค่าน้ำหนักบ่งบอกถึงความสำคัญ 

หากเราฝึกโมเดล LDA บนเอกสารที่เป็นข่าว 10 ข่าวและกำหนดให้แยกออกมาเป็น 3 หัว หัวเรื่องที่ได้ออกมาเป็นผลลัพท์อาจมีลักษณะดังนี้ 

| หัวเรื่อง | คำหลัก | 1 | 2 | 3 | 4 | 5 |
|---|-- |--|---|---|---|---|
| 1 |  |นายกรัฐมนตรี |รัฐบาล | รัฐสภา | ลงมติ | กรรมการ | 
|   | น้ำหนัก    | 0.3 | 0.2 | 0.1 | 0.1 | 0.05 |
| 2 | | ธุรกิจ | ชะลอตัว | ลด | การลงทุน | ดอกเบี้ย |
|   | น้ำหนัก    | 0.25 | 0.2 | 0.2 | 0.1 | 0.1 |
| 3 | | ฟุตบอล | บาสเกตบอล | กรรมการ | ชนะ | คะแนน |
|   | น้ำหนัก     | 0.1 | 0.1 | 0.1 | 0.05 | 0.025 |

หัวเรื่องที่ได้จาก LDA มักจะเรียกอีกอย่างหนึ่งว่าคำสำคัญของหัวเรื่อง (topic key) ซึ่งเราจะต้องอ่านและตีความว่าหัวเรื่องแต่ละหัวเรื่องเกี่ยวกับอะไรโดยการเชื่อมโยงหาธีมด้วยตัวเอง ตัวโมเดลเองไม่ได้ประกาศชัดเจนว่าหัวเรื่องที่ 1 2 หรือ 3 เกี่ยวกับอะไร จากตัวอย่างข้างต้นเราตีความว่าหัวเรื่องที่ 1 เกี่ยวกับการเมือง หัวเรื่องที่ 2 เกี่ยวกับเศรษฐกิจ และหัวเรื่องที่ 3 เกี่ยวกับกีฬา 

ในกระบวนการวิเคราะห์เดียวกัน LDA ยังจัดกลุ่มเอกสารโดยดูว่ามีเอกสารแต่ละเอกสารมีคำจากหัวเรื่องใดเป็นสัดส่วนเท่าไร ผลลัพธ์คือสัดส่วนของหัวเรื่องแต่ละหัวเรื่องในแต่ละเอกสาร ซึ่งเราจะเรียกว่าตาราง doc-topic เพราะว่าตารางผลลัพธ์จะมีเอกสารอยู่ที่แถวแต่ละแถว และมีหัวเรื่องอยู่ใน เช่น ถ้าหากเรามี 10 เอกสาร 

| เอกสาร | หัวเรื่อง 1 | หัวเรื่อง 2 | หัวเรื่อง 3 |
|---|---|---|---|
| 1 | 0.05 | 0.05 | 0.9 |
| 2 | 0.1 | 0.8 | 0.1 |
| 3 | 0.8 | 0.1 | 0.1 |
| 4 | 0.0 | 0.05 | 0.95 |
| 5 | 0.5 | 0.4 | 0.1 |
| 6 | 0.7 | 0.3 | 0.0 |
| 7 | 0.1 | 0.8 | 0.1 |
| 8 | 0.9 | 0.05 | 0.05 |
| 9 | 0.95 | 0.025 | 0.025 |
| 10 | 0.1 | 0.9 | 0.0 |

จากตัวอย่างผลลัพธ์ข้างต้น เอกสารที่ 3 5 7 และ 9 น่าจะเป็นข่าวการเมือง เนื่องจากมีสัดส่วนของหัวเรื่อง 1 มากที่สุด แต่เราสังเกตว่าเอกสารที่ 5 มีสัดส่วนของหัวเรื่อง 1 และ 2 พอ ๆ กัน ซึ่งอาจจะตีความได้ว่าเป็นเอกสารที่พูดถึงทั้งการเมืองและเศรษฐกิจ 
ส่วนเอกสารที่ 2 8 และ 10 น่าจะเป็นข่าวเศรษฐกิจ เนื่องจากมีสัดส่วนของหัวเรื่อง 2 มากที่สุด และเอกสารที่ 1 และ 4 น่าจะเป็นข่าวกีฬา เนื่องจากมีสัดส่วนของหัวเรื่อง 3 มากที่สุด  

สรุปแล้ว LDA ช่วยในการหาหัวเรื่องที่อยู่ในชุดเอกสาร และจัดกลุ่มเอกสารตามหัวเรื่องที่พบในเอกสารไปในคราวเดียวกัน โดยที่เราไม่ต้องระบุหัวเรื่องล่วงหน้า แต่เราต้องระบุจำนวนหัวเรื่องที่เราคิดว่าเหมาะสมกับชุดเอกสาร 
เมื่อฝึกโมเดลเสร็จแล้วเราสามารถตีความหัวเรื่องที่ได้จาก LDA ได้อย่างง่าย ๆ โดยการดูคำหลักที่เกี่ยวข้องกับหัวเรื่องนั้น และดูสัดส่วนของหัวเรื่องในแต่ละเอกสาร 

## ไลบรารี gensim สำหรับการฝึกโมเดล LDA

ไลบรารี gensim เป็นไลบรารีภาษาไพทอนที่เป็นที่นิยมมากสำหรับการใช้โมเดล LDA ไลบรารีนี้ทำให้การฝึกและใช้งานโมเดล LDA เป็นเรื่องง่ายและมีประสิทธิภาพ ไลบรารีนี้มีฟังก์ชันอำนวยความสะดวกในการเตรียมข้อมูลและการฝึกโมเดล gensim มีเครื่องมือที่ช่วยในการแปลงเอกสารให้เป็นเวกเตอร์และสร้างดิกชันนารีของคำ (dictionary) ซึ่งเป็นขั้นตอนที่สำคัญก่อนที่จะเริ่มฝึกโมเดล LDA นอกจากนี้ gensim ยังมีฟังก์ชันที่ช่วยในการปรับพารามิเตอร์ของโมเดลเพื่อให้ได้ผลลัพท์ที่ออกมามีประโยชน์ที่สุด 

ขั้นตอนแรกของการฝึกโมเดลคือการเปลี่ยนข้อมูลให้อยู่ในรูปของลิสต์ของเอกสาร ซึ่งเอกสารหนึ่งเอกสารจะต้องถูกเก็บอยู่ในรูปลิสต์ของคำที่ปราศจากคำหยุด เพราะฉะนั้นเราจะต้องทำการตัดคำ สมมติว่าเราต้องการวิเคราะห์ข้อมูลรีวิวสายการบิน ที่อยู่ในไฟล์ zip ที่มีชื่อไฟล์ว่า Airline_review.csv.zip ซึ่งมีคอลัมน์ *Review* ที่เก็บข้อมูลรีวิว ขั้นตอนแรกเราจะทำการอ่านข้อมูลจากไฟล์ zip และทำการตัดคำด้วยไลบรารี nltk และเก็บข้อมูลในรูปลิสต์ของคำ โดยเราจะใช้ฟังก์ชัน `clean` ที่เราได้สร้างขึ้นมาเพื่อทำความสะอาดข้อมูล และการตัดคำ 

```python
import pandas as pd
import re
from zipfile import ZipFile

zip_file_path = 'Airline_review.csv.zip'

# อ่าน csv ที่อยู่ในไฟล์ zip โดยที่ไม่ต้อง unzip ออกมาใส่เครื่องเพิ่ม
with ZipFile(zip_file_path, 'r') as zip_ref:
    with zip_ref.open('Airline_review.csv') as file:
        df = pd.read_csv(file)

def clean(text):
    # ลบตัวเลขและเครื่องหมายวรรคตอน
    text = re.sub('[^a-zA-Z\s]', '', text).lower()
    # ตัดคำ
    words = nltk.word_tokenize(text)
    # ต้องระบุคำหยุดใน stop_word_set ก่อน
    words = [x for x in words if x not in stop_word_set]
    return words

reviews = df['Review'].apply(clean).tolist()
```

โค้ดข้างต้นเรารันฟังก์ชัน `clean` ลงไปบนคอลัมน์ *Review* ที่อยู่ในดาตาเฟรม และเก็บใส่ตัวแปรชื่อว่า `reviews` ซึ่งมีโครงสร้างเป็นลิสต์ของลิสต์ของสตริง (คำ) ข้อมูลที่อยู่ในรูปนี้เป็นรูปแบบที่พร้อมใช้งานกับไลบรารี gensim และเราสามารถนำไปใช้ในการฝึกโมเดล LDA ได้เลย

```python
# use gensim to create a dictionary and a corpus
from gensim import corpora, models

dictionary = corpora.Dictionary(reviews)
corpus = [dictionary.doc2bow(review) for review in reviews]
lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, 
                            passes=5, alpha='auto', eta='auto',  random_state=10)
```

ในโค้ดข้างต้นเราใช้คลาส `gensim.corpora.Dictionary` ในการสร้างดิกชันนารีของคำ และใช้เมท็อด `doc2bow` ในการแปลงข้อมูลให้อยู่ในรูปของลิสต์ของตัวเลข และเก็บใส่ตัวแปร `corpus` ซึ่งเป็นลิสต์ของลิสต์ของตัวเลข หลังจากนั้นเราใช้คลาส `gensim.models.LdaModel` ในการฝึกโมเดล LDA พารามิเตอร์ที่สำคัญที่สุดคือ จำนวนหัวเรื่อง (`num_topics`) ที่เราต้องการให้โมเดลหา ส่วนพารามิเตอร์อื่น ๆ ที่เกี่ยวข้องกับการฝึกโมเดลสามารถตั้งตามที่เห็นตัวอย่างได้เลย เช่น จำนวนรอบการฝึก (`passes`) เป็น 5 และการกำหนดค่าพารามิเตอร์ `alpha` และ `eta` โดยเราสามารถใช้ค่า `auto`  ส่วน `random_state` สามารถตั้งค่าเป็นอะไรก็ได้ การฝึกโมเดล LDA มีการต้องสุ่มค่าเริ่มต้นการฝึก เพราะฉะนั้นหากเราฝึกโมเดลใหม่อีกครั้งโดยใช้ค่า `random_state` เดียวกันก็จะได้ผลออกมาเหมือนเดิมทุกอย่าง หากไม่ระบุเลยแล้วกดรันอีกครั้งหนึ่งผลจะออกมาแตกต่าง 

 หลังจากฝึกโมเดลเสร็จเราสามารถดูหัวเรื่องที่ได้จากโมเดลได้ด้วยเมท็อด `show_topic` ซึ่งจะคืนค่าเป็นลิสต์ของทูเปิล (คำ, ค่าน้ำหนัก) เรียงลำดับตั้งแต่มากไปน้อย ตัวอย่างเช่น หากเราเรียก `lda_model.show_topic(0)` จะได้ผลลัพธ์ดังนี้

 ```python
 [('flight', 0.035730693),
 ('good', 0.017981019),
 ('food', 0.016260142),
 ('crew', 0.016218152),
 ('service', 0.01401287),
 ('cabin', 0.013283503),
 ('time', 0.011989626),
 ('seats', 0.00853041),
 ('friendly', 0.008275962),
 ('staff', 0.00817701)]
 ```

หากต้องการดูทุกหัวเรื่องโดยดูเฉพาะคำที่มีค่าน้ำหนักสูงสุด 10 คำแรก gensim ไม่ได้มีเมท็อดที่แสดงผลออกมาให้ดูง่าย เราจึงต้องเขียนลูปดังนี้

```python
for i in range(lda_model.num_topics):
    keyword_list = [w[0] for w in lda_model.show_topic(i)]
    print (f'หัวเรื่อง {i}: {" ".join(keyword_list)}')
```
ผลลัพธ์ที่ได้จะมีลักษณะดังนี้

```
หัวเรื่อง 0: flight good food crew service cabin time seats friendly staff
หัวเรื่อง 1: luggage bag baggage pay kg extra carry bags checked wow
หัวเรื่อง 2: de la que volotea el en las con por un
หัวเรื่อง 3: airline air flights airlines service fly time like flying low
หัวเรื่อง 4: seat seats extra flight row paid get front plane back
หัวเรื่อง 5: luggage baggage lost bag bags claim days manila arrived suitcase
หัวเรื่อง 6: ticket pay check service online customer airline website said told
หัวเรื่อง 7: johannesburg china kuala lumpur test town cape klm chinese shanghai
หัวเรื่อง 8: flight us plane airport boarding staff time gate passengers minutes
หัวเรื่อง 9: flight airline hours get us service customer day time would
```

จากนั้นเราต้องนำหัวเรื่องแต่ละหัวเรื่องมาตีความต่อ

- หัวเรื่อง 0: บริการบนเครื่องบิน
- หัวเรื่อง 1: การจ่ายค่าบริการเกี่ยวกับน้ำหนักกระเป๋าที่สามารถนำขึ้นเครื่องได้
- หัวเรื่อง 2: เป็นคำศัพท์ภาษาสเปนที่ส่วนใหญ่เป็นคำหยุด ทำให้เห็นว่าข้อมูลเราอาจจะมีข้อมูลที่ไม่เป็นภาษาสเปนติดมาด้วย เราควรจะกลับไปกรองออกก่อนให้เรียบร้อย
- หัวเรื่อง 3: บริการของสายการบิน
- หัวเรื่อง 4: ที่นั่งบนเครื่องบิน
- หัวเรื่อง 5: การสูญหายของกระเป๋า
- หัวเรื่อง 6: การซื้อตั๋วและการบริการลูกค้าออนไลน์ ผ่านหน้าเว็บไซต์
- หัวเรื่อง 7: สถานที่เป็นปลายทางการบิน
- หัวเรื่อง 8: ประสบการณ์การเช็คอินและขึ้นเครื่อง
- หัวเรื่อง 9: ระยะเวลาที่แผนกบริการลูกค้าใช้ในการตอบกลับลูกค้า

ถ้าหากว่าผลลัพธ์ออกมาไม่ดีเท่าทีควร มีทางแก้อยู่สองทางด้วยกัน 

1. ลองเปลี่ยนจำนวนหัวเรื่อง ถ้าจำนวนหัวเรื่องมากเกินไป เราจะพบเห็นหัวเรื่องที่ซ้ำ ๆ กัน ถ้าจำนวนหัวเรื่องน้อยเกินไป เราจะพบเห็นหัวที่มีคำที่ไม่เกี่ยวข้องกันอยู่ด้วย ดังนั้นเราควรจะลองเปลี่ยนจำนวนหัวเรื่องใหม่อีกครั้ง

2. ทำความสะอาดข้อมูลให้ดีขึ้น หากเจอคำที่เกิดขึ้นในหลาย ๆ หัวเรื่องที่ไม่ได้มีประโยชน์ในการตีความ ทำความเข้าใจหัวเรื่อง อาจจะสกัดเอาคำเหล่านั้นออกไป เช่น ในตัวอย่างข้างต้น เราอาจจะเอาคำว่า *flight* และ *flights* ออกไปเนื่องจาก เราทราบอยู่แล้วว่าข้อมูลเกี่ยวกับสายการบิน คำว่า *flight* ย่อมปรากฏอยู่ในทุกรีวิว และไม่มีประโยชน์ในการตีความหัวเรื่อง

## สรุป

ในบทนี้เราเรียนรู้เทคนิควิธีการประมวลผลภาษาธรรมชาติที่ใช้บ่อย ๆ ได้แก่ การติดป้ายกำกับชนิดของคำ การรู้จำเอนทิตี การวิเคราะห์อารมณ์ความรู้สึก การวิเคราะห์หัวเรื่องที่ปรากฏในชุดเอกสาร ในปัจจุบันมีไลบรารีที่สามารถดึงโมเดลต่าง ๆ ลงมาใช้ในภาษาไพทอนได้สะดวก เนื่องจากมีซอฟต์แวร์ที่เป็นโอเพนซอร์สมากมายที่ได้รับการพัฒนาอย่างต่อเนื่อง ทำให้สามารถทำการประมวลผลภาษาธรรมชาติได้ง่ายขึ้น หรือสามารถเทรนโมเดลบนชุดข้อมูลของเราเองอย่างการทำโมเดลหัวเรื่องได้ 

## อ้างอิง
```{bibliography}
:filter: docname in docnames
:style: plain

```