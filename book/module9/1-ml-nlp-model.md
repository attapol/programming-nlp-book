# บทที่ 9 </br> โมเดลการประมวลผลภาษาธรรมชาติ

การประมวลผลภาษาธรรมชาติ คือ เทคนิควิธีที่ใช้ในการประมวลผลและทำความเข้าใจข้อมูลตัวอักษร โดยอาศัยโมเดลทางภาษา โมเดลหรือโมเดลที่เราพูดถึงนี้หมายถึง การเขียนโปรแกรมที่จำลองการทำความเข้าใจภาษาในด้านต่าง ๆ ซึ่งการจำลองอาจจะไม่ได้สอดคล้องกับวิธีที่มนุษย์ทำความเข้าใจภาษา แต่ว่าโมเดลจำลองพฤติกรรมการประมวลผลภาษาได้ดีพอ ที่จะนำไปใช้ในการทำงานที่เกี่ยวข้องกับภาษาธรรมชาติได้ 
งานที่สามารถใช้โมเดลการประมวลผลภาษาธรรมชาติได้อย่างมีประสิทธิภาพ และพบเห็นได้บ่อย ๆ ได้แก่  เช่น การแท็กชนิดของคำ (part-of-speech tagging) การตรวจจับเอนติตี้ (named entity recognition) การวิเคราะห์อารมณ์ความรู้สึก (sentiment analysis) โมเดลหัวข้อ (topic model) การแปลด้วยเครื่อง (machine translation) ซึ่งทั้งหมดนี้คือต่างใช้การเรียนรู้ของเครื่อง (machine learning) ในการสร้างโมเดลสำหรับงานเหล่านี้ขึ้นมา กล่าวคือเราเขียนโปรแกรมที่เรียนรู้การทำงานเหล่านี้โดยการหาแพทเทิร์นจากชุดข้อมูลที่เหมาะสม เราไม่ได้ตั้งโปรแกรมให้ทำงานตามกฎของภาษาศาสตร์โดยตรง 

ในปัจจุบันเราสามารถดาวน์โหลดโมเดลและนำมาประยุกต์ใช้ได้ทันทีโดยที่ต้องไม่ทำการสร้างโมเดลใหม่ตั้งแต่ต้น แต่ว่าอาจจะมีโมเดลสำหรับบางงานและบางภาษาเท่านั้น และโมเดลของแต่ละภาษามีประสิทธิภาพ และความแม่นยำแตกต่างกันไป ในบทนี้เราจะเรียนรู้วิธีการใช้ไลบรารีเพื่อดาวน์โหลดและประยุกต์ใช้โมเดลในการวิเคราะห์ข้อมูลภาษาโดยอัตโนมัติ

## การแท็กชนิดของคำ (Part-of-speech tagging) 

ชนิดของคำ (part-of-speech tag) ในเชิงภาษาศาสตร์ถูกสร้างขึ้นมาเป็นส่วนหนึ่งของกฎไวยากรณ์ของภาษาที่กำหนดว่าคำชนิดใดสามารถมีความสัมพันธ์กับคำชนิดใด ในลักษณะไหนได้บ้าง  นอกจากนั้นแล้วยังมีบทบาทสำคัญในการทำความเข้าใจความหมายและโครงสร้างของประโยค
เนื่องจากช่วยให้เรารู้ว่าแต่ละคำในประโยคทำหน้าที่อะไร  เช่น คำนาม (noun) มักจะทำหน้าที่เป็นประธานของประโยค หรือเป็นส่วนเติมเต็มของกริยา หรีอ คุณศัพท์ (adjective) มักจะทำหน้าที่เป็นตัวขยายความหมายของคำนาม เป็นต้น 

```{margin} คำศัพท์
ชนิดของคำ (part-of-speech tag)
```

ในการประมวลผลภาษาธรรมชาติ ชนิดของคำมักจะช่วยในการสกัดข้อมูล (information extraction) จากข้อมูลที่เป็นข้อความ เช่น ถ้าหากเราต้องการสกัดเอาชื่อของบุคคลจากข้อความ เราอาจจะเลือกเอาเฉพาะคำที่เป็นคำนามมาพิจารณาเท่านั้น หรือถ้าหากเราต้องการสกัดคีย์เวิร์ดที่บ่งบอกถึงความหมายใจความสำคัญของคลังเอกสาร เราอาจจะเลือกเฉพาะไบแกรมที่มีคำนามและคำกริยามาพิจารณาเท่านั้น 


```{margin} คำศัพท์
การสกัดข้อมูล (information extraction)
```

### การแยกชนิดของคำ

ชนิดของคำมีชนิด ชนิดอะไรบ้าง ทฤษฎีการแยกชนิดของคำมีอยู่หลากหลายทฤษฎี เช่น เด็กนักเรียนไทยมักจะคุ้นเคยกับการแยกชนิดของคำตามพระยาอุปกิตศิลปสารซึ่งแยกชนิดของคำเป็น 7 ชนิด ได้แก่ คำนาม คำสรรพนาม คำวิเศษณ์ คำกริยา คำบุพบท คำสันธาน และคำอุทาน  ในการประมวลผลภาษาธรรมชาติ เรามักใช้ทฤษฎีของ ชุดป้ายระบุชนิดของคำสากล (Universal Part-of-speech Tagset) ทฤษฎีนี้กำหนดชนิดของคำออกเป็น 17 ชนิด ซึ่งสามารถจัดออกเป็น 3 กลุ่มใหญ่ ได้แก่ 

```{margin} คำศัพท์
ชุดป้ายระบุชนิดของคำสากล (Universal Part-of-speech Tagset) 
```


1. คำชนิดเปิด (open class word) คือกลุ่มคำที่สามารถเพิ่มคำใหม่เข้าไปได้เรื่อยๆ คำชนิดเปิดมักมีความหมายที่ชัดเจนและสามารถยืนอยู่ได้ด้วยตัวเอง เช่น การกระทำ คน สัตว์ สิ่งของ สถานที่ คำชนิดเปิดประกอบด้วยคำ 6 ชนิดย่อย
    - คำวิเศษณ์ (adverb)
    - คำคุณศัพท์ (adjective)
    - คำอุทาน (interjection)
    - คำนาม (noun)
    - คำกริยา (verb)
    - วิสามานยนาม หรือคำนามชื่อเฉพาะ (proper noun)
2. คำชนิดปิด (closed class word) เป็นกลุ่มคำที่มีจำนวนจำกัดและไม่ค่อยมีการเพิ่มคำใหม่ หน้าที่หลักของคำชนิดปิดคือการเชื่อมโยงหรือจัดระเบียบความสัมพันธ์ระหว่างคำในประโยค คำชนิดปิดมักมีความหมายที่น้อยลงหรือเป็นนามธรรม และมักไม่สามารถยืนอยู่ได้ด้วยตัวเอง แต่ต้องใช้งานร่วมกับคำอื่นเพื่อสร้างประโยคที่ถูกหลักไวยากรณ์ คำชนิดปิดประกอบด้วยคำ 8 ชนิดย่อย  
    - คำบุพบท (adposition)
    - คำช่วยกริยา (auxiliary)
    - ตัวกำหนด (determiner)
    - ตัวเลข (numeral)
    - คำอนุภาค (particle)
    - คำสรรพนาม (pronoun)
    - คำสันธานเชื่อมความ (coordinating conjunction)
    - คำสันธานซ้อนความ (subordinating conjunction)
3. คำชนิดอื่น ๆ ประกอบด้วยคำ 3 ชนิดย่อย 
    - เครื่องหมายวรรคตอน (punctuation)
    - สัญลักษณ์ (symbol)
    - อื่น ๆ ที่ไม่เข้าพวกกับคำชนิดที่กล่าวมาแล้ว

 {cite}`petrov-etal-2012-universal`     

ชุดป้ายระบุชนิดของคำสากลเป็นระบบการจำแนกชนิดของคำที่มีมาตรฐานเดียวกันทุกภาษา เพื่อให้การทำงานของโมเดลภาษาต่าง ๆ สามารถทำงานร่วมกันได้โดยสะดวกต่อการใช้โมเดลที่มีการใช้การถ่ายโอนข้ามภาษา (cross-lingual transfer) การใช้ชุดป้ายระบุชนิดของคำสากล จึงเป็นเครื่องมือที่สำคัญและได้รับความนิยมในการพัฒนาระบบประมวลผลภาษาธรรมชาติที่มีประสิทธิภาพ

## การตรวจจับเอนทิตี (Named Entity Recognition)

เอนทิตีในเชิงทฤษฎีแล้วคือ สิ่งที่มีตัวตนจริงและอิสระไม่เกี่ยวพันกับสิ่งอื่น ๆ ซึ่งเรามักจะหมายถึงบุคคล องค์กร บริษัท หรือสถานที่ เพราะฉะนั้นคำว่า named entity หมายความถึง บุคคล องค์กร สถานที่ หรือสิ่งที่มีตัวตนประเภทอื่น ๆ ที่มีสามารถอ้างถึงได้ด้วยชื่อ  ในบริบทของการประมวลผลภาษาธรรมชาติการตรวจจับเอนทิตี (Named Entity Recognition หรือ NER) คือ การตรวจจับชื่อของบุคคล ชื่อขององค์กร ชื่อของสถานที่ ชื่อของเหตุการณ์ หรือชื่อของสิ่งของที่มีความสำคัญออกมาจากข้อความ และในทางปฏิบัติแล้วการตรวจจับเอนทิตีไม่ได้จำกัดเพียงแต่เอนทิตีอย่างเดียว แต่หมายความรวมถึงการตรวจจับส่วนอื่นของข้อความที่มีความสำคัญ​และอาจจะไม่ได้มีตัวตนอิสระตามความหมายของเอนทิตี เช่น

```{margin} คำศัพท์
เอนทิตี (entity)
```

- เวลาและวันที่ เช่น "เมื่อวาน" หรือ "สัปดาห์หน้า"
- จำนวนเงิน เช่น "500 บาท" หรือ "หนึ่งพันเหรียญสหรัฐ"
- จำนวนและขนาด เช่น "3 กิโลเมตร" หรือ "5 ลิตร"
- คำนำหน้าชื่อ และคำศัพท์ทั่วไปที่เป็นคำบ่งบอกสถานที่ เช่น "เด็กชาย" "นางสาว" หรือ "โรงเรียน" ซึ่งไม่ได้บ่งบอกถึงชื่อเฉพาะของบุคคลหรือสถานที่

การตรวจจับเอนทิตีมีประโยชน์อย่างมากในหลายด้านของการประมวลผลภาษาธรรมชาติ และการประยุกต์ใช้ในงานต่างๆ โดยสามารถยกตัวอย่างได้ดังนี้ 

- ในการวิเคราะห์ความเห็นของลูกค้าเกี่ยวกับผลิตภัณฑ์ในโซเชียลมีเดีย บริษัทสามารถใช้การตรวจจับเอนทิตีเพื่อระบุชื่อผลิตภัณฑ์ ชื่อบริษัท หรือบุคคลที่เกี่ยวข้อง จากนั้นสามารถวิเคราะห์ความเห็นในเชิงบวกหรือลบที่มีต่อเอนทิตีเหล่านี้ได้อย่างมีประสิทธิภาพ ซึ่งช่วยให้บริษัทสามารถเข้าใจความคิดเห็นของลูกค้าเกี่ยวกับผลิตภัณฑ์หรือบริการได้อย่างละเอียด และสามารถปรับปรุงผลิตภัณฑ์และการบริการตามความคิดเห็นที่ได้รับได้อย่างตรงจุด 
- ในระบบตอบคำถามอัตโนมัติ เช่น ระบบบริการหลังการขายออนไลน์  การตรวจจับเอนทิตีสามารถระบุข้อมูลสำคัญจากคำถามของผู้ใช้ เช่น ชื่อผลิตภัณฑ์ หรือปัญหาที่เกิดขึ้น ทำให้ระบบสามารถให้คำตอบที่แม่นยำและรวดเร็วแก่ผู้ใช้ ลดเวลาที่ต้องใช้ในการค้นหาข้อมูลและเพิ่มประสิทธิภาพในการให้บริการลูกค้า
- ในด้านการสกัดข้อมูลทางการแพทย์ การตรวจจับเอนทิตีมีบทบาทสำคัญในการวิเคราะห์เวชระเบียนหรือเอกสารทางการแพทย์ โดยสามารถระบุชื่อโรค ชื่อยา ชื่อแพทย์ หรือวันนัดหมายได้อย่างชัดเจน ซึ่งช่วยในการจัดการข้อมูลทางการแพทย์อย่างเป็นระบบ สามารถสกัดข้อมูลสำคัญเพื่อใช้ในการวินิจฉัยโรค หรือการวิจัยทางการแพทย์ได้อย่างรวดเร็วและแม่นยำ 
- ในการวิเคราะห์ข่าวสารจากแหล่งต่างๆ การตรวจจับเอนทิตีสามารถระบุชื่อบุคคลสำคัญ เหตุการณ์สำคัญ สถานที่ หรือชื่อองค์กรที่ปรากฏในข่าว ทำให้สำนักข่าวหรือองค์กรที่เกี่ยวข้องสามารถติดตามและวิเคราะห์ข้อมูลข่าวสารได้อย่างมีประสิทธิภาพ สามารถสรุปแนวโน้มข่าวสาร หรือทำการวิจัยเชิงลึกเกี่ยวกับประเด็นที่สำคัญได้ 


การตรวจจับเอนทิตีมักจะใช้เทคนิคการประมวลผลภาษาธรรมชาติเช่นเดียวกับการแยกชนิดของคำ โดยใช้โมเดลการเรียนรู้ของเครื่องที่เรียนรู้จากชุดข้อมูลที่มีเอนทิตีที่ถูกติดป้ายไว้ และใช้เอนทิตีที่ติดป้ายเหล่านั้นในการสร้างโมเดลที่สามารถตรวจจับเอนทิตีใหม่ได้

## ไลบรารี spacy สำหรับการติดป้ายชนิดของคำ และการตรวจจับเอนทิตี

การติดป้ายชนิดของคำ และการตรวจจับเอนทิตีอัตโนมัติจำเป็นต้องใช้โมเดลแบบการเรียนรู้ของเครื่องในการแยก เพราะฉะนั้นไลบรารีที่ใช้จะต้องมีโมเดลเหล่านี้เตรียมไว้ให้พร้อมใช้ ไลบรารี spacy เป็นไลบรารีที่รองรับการประมวลผลภาษาธรรมชาติหลากหลายภาษา รวมถึงมีโมเดลการประมวลผลภาษาธรรมชาติหลากหลายขนาดให้เลือกใช้ตามความเหมาะสม และตามข้อจำกัดในการนำโมเดลไปใช้จริง  {cite}`spacy` ปัจจุบัน (2024) spacy รองรับการติดป้ายชนิดของคำสำหรับภาษาหลายภาษา เช่น ภาษาอังกฤษ ภาษาสเปน ภาษาฝรั่งเศส ภาษาจีน ภาษาญี่ปุ่น ภาษาเกาหลี แต่ว่า spacy ยังไม่มีโมเดลในการประมวลผลภาษาไทย

ขั้นแรกต้องหาชื่อชุดโมเดลที่เราต้องการใช้ รายชื่อชุดโมเดลอยู่บนเว็บไซต์ของ spacy ซึ่งมีเอกสารประกอบการใช้งานที่เป็นปัจจุบันอยู่ โมเดลเหล่านี้ประกอบไปด้วยโมเดลย่อยหลาย ๆ โมเดลด้วยกัน เราสามารถตรวจสอบดูบนเว็บไซต์ได้ว่าโมเดลแต่ละชุดประกอบด้วยโมเดลอะไรบ้าง  ตัวอย่างเช่น สำหรับภาษาจีน มีชุดโมเดลที่มีโมเดลการแยกชนิดของคำได้อยู่สามโมเดลได้แก่ `zh_core_web_sm` (ขนาดเล็ก) `zh_core_web_md` (ขนาดกลาง) และ  `zh_core_web_lg` (ขนาดใหญ่) ทั้งสามชุดในเวอร์ชันปี 2024 ประกอบไปด้วย 5 โมเดลย่อย ได้แก่ โมเดลการตัดคำ โมเดลการตัดประโยค โมเดลการวิเคราะห์โครงสร้างประโยค โมเดลการติดป้ายชนิดของคำ โมเดลการตรวจจับเอนทิตี  

หากเรายังไม่ได้ติดตั้งไลบรารี spacy เราสามารถติดตั้งโดยใช้คำสั่ง `pip install spacy` หลังจากนั้นเราสามารถดาวน์โหลดและติดตั้งโมเดลที่ต้องการได้โดยใช้คำสั่ง `python -m spacy download` ตามด้วยชื่อโมเดลที่ต้องการดาวน์โหลดและติดตั้ง เช่น

```bash
python -m spacy download zh_core_web_md
```

วิธีการใช้ spacy จะต่างจากการใช้ pythainlp ตรงที่ว่าเราจะต้องสร้างอ็อบเจกต์ที่ทำหน้าที่เป็นตัวประมวลผลที่โหลดโมเดลที่ต้องการมาให้พร้อม เพื่อที่จะได้ไม่ต้องโหลดโมเดลซ้ำ ๆ บ่อย ๆ เพราะถ้าหากเป็นโมเดลขนาดใหญ่แล้วต้องโหลดโมเดลซ้ำ ๆ จะทำให้โปรแกรมทำงานช้า ไม่เหมาะกับการประมวลผลข้อมูลขนาดใหญ่ที่เราอาจจะต้องลูปวนเพื่อประมวลผลข้อมูลทีละชิ้น ตัวอย่างการใช้งาน spacy ในการแยกชนิดของคำแสดงในโค้ดต่อไปนี้

```python 
import spacy
model_name = 'zh_core_web_md'
nlp_processor = spacy.load(model_name)
text  = '张伟和李娜在漂亮的上海吃饭'
doc = nlp_processor(text)
for token in doc:
    print(token.text, token.pos_)
```

ผลลัพธ์ที่ได้จะแสดงชนิดของคำที่แยกออกมาจากข้อความ ตามที่กำหนดไว้ในชุดป้ายระบุชนิดของคำสากลดังนี้

```
张伟 PROPN
和 CCONJ
李娜 PROPN
在 ADP
漂亮 VERB
的 PART
上海 PROPN
吃饭 VERB
```

 ในตัวอย่างนี้เราโหลดโมเดล `zh_core_web_md` ซึ่งเป็นโมเดลขนาดกลางสำหรับภาษาจีน โดยการเรียกคำสั่ง `spacy.load` เพื่อสร้างตัวประมวลผลข้อความและเก็บไว้ในตัวแปรชื่อ `nlp_processor` ซึ่งสามารถใช้ได้คล้ายกับว่าเป็นฟังก์ชัน เราจึงสามารถเรียก `nlp_processor(text)` ได้เลยโดยไม่ต้องใช้ชื่อเมท็อดอื่น ผลที่ได้คือเป็น `Doc` อ็อบเจกต์ที่เราสามารถวนลูปเพื่อดึง `Token` อ็อบเจกต์ออกมาและดึงชนิดของคำออกมาได้โดยใช้ `token.pos_` ในการเข้าถึงชนิดของคำ


 ข้อสังเกตที่สำคัญอีกประการหนึ่งของการใช้ spacy คือ ไลบรารีนี้ประมวลข้อความเป็นแบบการทำงานแบบสายท่อ (pipeline) กล่าวคือสตริงที่รับเข้ามาจะถูกประมวลหลาย ๆ ขั้นในครั้งเดียว เช่น การประมวลภาษาจีนจะเริ่มด้วยการตัดคำ ต่อด้วยการแยกชนิดของคำ การวิเคราะห์โครงสร้างประโยค และจบด้วยการตรวจจับเอนทิตี จากนั้นผู้ใช้สามารถเข้าถึงผลการวิเคราะห์ทั้งหมดนี้ผ่าน `Doc` อ็อบเจกต์ที่ได้รับกลับมาจากการเรียก `nlp_processor(text)` โดยไม่ต้องเรียกแต่ละขั้นตอนเอง  เพราะฉะนั้นเราสามารถเข้าถึงรายชื่อเอนทิตีได้เลย โดยไม่ต้องประมวลผลข้อความอีกครั้ง

 ```python
for ent in doc.ents:
    print(ent.text, ent.label_)
```

ผลลัพท์ที่ได้จะแสดงชื่อเอนทิตีที่ตรวจจับออกมาจากข้อความดังนี้

```
张伟 PERSON
李娜 PERSON
上海 GPE
```
จากข้อความตัวอย่างที่ป้อนเข้าไป ตัวประมวลผลตรวจจับชื่อเอนทิตีที่เป็นบุคคลทั้งหมดสองชื่อ ได้แก่  张伟 (จางเหว่ย) และ 李娜 (หลีน่า) และเอนทิตีแบบภูมิรัฐศาสตร์ (GPE ซึ่งย่อมาจาก geopolitical entity) หนึ่งชื่อซึ่งก้คือ 上海 (เมืองเซี่ยงไฮ้)


## การวิเคราะห์อารมณ์ความรู้สึก (Sentiment Analysis)

การวิเคราะห์อารมณ์ความรู้สึก คือกระบวนการในการตรวจจับและจำแนกอารมณ์หรือความรู้สึกที่แสดงออกมาในข้อความ โดยใช้เทคนิคทางด้านการประมวลผลภาษาธรรมชาติ และการเรียนรู้ของเครื่อง การวิเคราะห์อารมณ์ความรู้สึกมักถูกใช้เพื่อประเมินความคิดเห็นของผู้ที่ใช้งานระบบ หรือผู้บริโภคที่มีต่อผลิตภัณฑ์ บริการ รวมถึงการประเมินความคิดเห็นของบุคคลทั่วไปที่มีต่อเหตุการณ์ต่าง ๆ โดยสามารถจำแนกออกเป็นอารมณ์เชิงบวก เชิงลบ หรือเป็นกลาง การวิเคราะห์อารมณ์ความรู้สึกนับว่าเป็นการประยุกต์ใช้การประมวลผลภาษาธรรมชาติในโลกของวิทยาการข้อมูลที่เห็นเด่นชัด และเป็นที่นิยมที่สุดชนิดหนึ่งในปัจจุบัน ตัวอย่างของการวิเคราะห์อารมณ์ความรู้สึกมีอยู่หลากหลายแบบ เช่น

```{margin} คำศัพท์
การวิเคราะห์อารมณ์ความรู้สึก (sentiment analysis) 
```

- การวิเคราะห์ความคิดเห็นของลูกค้าในโซเชียลมีเดีย: สมมติว่าเรามีข้อความจากทวิตเตอร์เกี่ยวกับผลิตภัณฑ์ใหม่ของบริษัท เราสามารถแยกแยะความคิดเห็นเชิงบวก ลบ หรือกลาง และนับจำนวนความคิดเห็นแต่ละประเภทได้ และแยกวิเคราะห์ได้ บริษัทสามารถใช้ข้อมูลนี้เพื่อเข้าใจความคิดเห็นของลูกค้าต่อผลิตภัณฑ์ใหม่ และนำไปปรับปรุงผลิตภัณฑ์หรือกลยุทธ์การตลาดได้

- การประเมินความพึงพอใจของลูกค้าในบทวิจารณ์ออนไลน์:
การวิเคราะห์อารมณ์ความรู้สึกสามารถแยกแยะได้ว่าอาจจะมีความรู้สึกเชิงบวกต่อผลิตภัณฑ์ แต่มีความรู้สึกเชิงลบต่อการบริการ ร้านอาหารสามารถใช้ข้อมูลนี้เพื่อปรับปรุงการบริการและรักษาคุณภาพอาหารให้ดีอยู่เสมอ

- การวิเคราะห์อารมณ์ความรู้สึกที่อยู่เอกสารทางการเงิน ทุก ๆ ปีบริษัทที่มีการซื้อขายหุ้นอยู่ในตลาดหลักทรัพย์ จะต้องเผยแพร่เอกสารรายงานผลประกอบการที่จะต้องพูดถึงบริษัทในหลากหลายด้าน เช่น เรื่องผลกำไรขาดทุน ความเสี่ยงในการประกอบธุรกิจ การควบรวมกับบริษัทอื่น การพัฒนากำลังคน หรือการเปลี่ยนแปลงในโครงสร้างบริษัท นักวิเคราะห์สามารถวิเคราะห์อารมณ์ความรู้สึกในเอกสารรายงานผลประกอบการ เพื่อให้ได้ดัชนีในการชีวัดว่าผู้เขียนมีความคิดเห็นในด้านบวก ลบ หรือกลางในด้านใดบ้างในปีที่ผ่านมา ซึ่งมีการศึกษามาว่าผลการวิเคราะห์ในลักษณะดังกล่าวมีความสัมพันธ์ร่วมกันกับผลตอบแทนเกินปกติ (abnormal return) ดังนั้นการวิเคราะห์อารมณ์ความรู้สึกจึงสามารถช่วยให้นักลงทุนหรือผู้บริหารบริษัทมีดัชนีชี้วัดเพิ่มอีกตัวหนึ่งที่ช่วยในการตัดสินใจลงทุนในสินทรัพย์ต่าง ๆ ได้อย่างมีประสิทธิภาพมากขึ้น 

- การติดตามความคิดเห็นเรื่องการเมืองบนสื่อออนไลน์: การศึกษาเรื่องวิเคราะห์อารมณ์ความรู้สึกที่อยู่บนสื่อออนไลน์ เช่น เว็บไซต์ reddit สามารถช่วยให้นักวิเคราะห์เข้าใจความคิดเห็นของประชาชนต่อเรื่องการเมือง เนื่องจากเหตุการณ์ทางการเมืองต่าง ๆ ส่งผลต่ออารมณ์ความรู้สึกที่ปรากฏอยู่ในสื่อเหล่านี้ {cite}`tachaiya2021raffman` การวิเคราะห์อารมณ์ความรู้สึกบนฟอรัมออนไลน์มีประโยชน์อย่างมากในการทำความเข้าใจและตอบสนองต่อความคิดเห็นของประชาชนได้อย่างมีประสิทธิภาพ โดยช่วยให้เราสามารถติดตามการเปลี่ยนแปลงของอารมณ์ความรู้สึกในช่วงเวลาต่างๆ เมื่อเกิดเหตุการณ์สำคัญ เช่น การถอดถอนประธานาธิบดี ซึ่งจะเป็นประโยชน์ในการระบุแนวโน้มทางการเมืองและสังคม นอกจากนี้ ยังสามารถช่วยในการวิเคราะห์แนวโน้มการแบ่งขั้วทางการเมืองหรือความรู้สึกที่มีต่อประเด็นสำคัญในสังคมอีกด้วย 

การวิเคราะห์อารมณ์ความรู้สึกในเชิงปฏิบัติ คือ การแยกประเภทของข้อความว่าเป็นข้อความที่แสดงความเห็นในด้านบวก ด้านลบ หรือเป็นกลาง เราสามารถคิดว่าตัววิเคราะห์อารมณ์ความรู้สึก (sentiment analyzer) เป็นฟังก์ชันที่รับสตริงข้อความ และคืนค่าเป็นป้ายกำกับ (label) ที่บ่งบอกว่าข้อความนั้นเป็นเชิงบวก ลบ หรือเป็นกลาง โดยใช้เทคนิคการเรียนรู้ของเครื่อง ซึ่งสามารถทำได้ด้วยการใช้ชุดข้อมูลที่มีป้ายระบุความรู้สึกเช่นข้อความที่มีป้ายระบุว่าเป็นเชิงบวก ลบ หรือเป็นกลาง ในการสร้างโมเดลการเรียนรู้ของเครื่อง โดยใช้ข้อมูลเหล่านี้ในการเรียนรู้ว่าคุณลักษณะของข้อความที่เป็นเชิงบวก ลบ หรือกลาง คืออะไร และใช้คุณลักษณะเหล่านั้นในการจำแนกประเภทของข้อความใหม่ที่ยังไม่เคยเห็นมาก่อน หนึ่งในชุดข้อมูลที่ได้รับความนิยมมากในการนำมาสร้างตัววิเคราะห์อารมณ์ความรู้สึก คือ Sentiment Treebank  ซึ่งนำมาแสดงให้ดูเป็นตัวอย่างดังนี้ โดยเลือกเฉพาะตัวอย่างที่มีคำว่า *excellent* ซึ่งแปลว่ายอดเยี่ยมอยู่อย่างน้อย 1 จุด {cite}`sentimenttreebank`

|  | ข้อความ | ป้ายกำกับ |
|---|---------|-------------------|
|(a) | Leguizamo and Jones are both excellent and the rest of the cast is uniformly superb. | บวก |
|(b) | Still, this flick is fun, and host to some truly excellent sequences. | บวก |
|(c) | A potentially good comic premise and excellent cast are terribly wasted. | ลบ |
|(d)| There's an excellent 90-minute film here; unfortunately, it runs for 170 | ลบ | 
|(e)| So brisk is Wang's pacing that none of the excellent cast are given air to breathe. | เป็นกลาง |

ข้อสังเกตที่สำคัญอย่างหนึ่งจากตัวอย่างข้างต้น คือ ถึงแม้ว่าทุกข้อความจะมีคำว่า *excellent* อยู่  แต่ก็ไม่ได้หมายความว่าอารมณ์ความรู้สึกที่ส่งผ่านข้อความนั้นจะเป็นบวกเสมอไป ยังมีปัจจัยทางบริบทที่มีผลต่อการตีความโดยรวม เช่น บริบททางโครงสร้างประโยคในตัวอย่าง (c) นำเอาคำว่า *excellent* มาใช้เป็นส่วนขยายของนามแต่ว่าไม่ได้นำมาใช้เป็นส่วนหนึ่งของกริยาหลัก ซึ่งมีผลต่อความหมายโดยรวมของประโยคมากกว่า ทำให้เห็นว่าเราต้องคำนึงปัจจัยทางภาษาศาสตร์อื่น ๆ อีกมาก นอกเหนือจากความหมายของคำ จึงจะสามารถวิเคราะห์อารมณ์ความรู้สึกได้ถูกต้อง

## แพลตฟอร์ม huggingface และไลบรารี transformers สำหรับการวิเคราะห์อารมณ์ความรู้สึก 

Huggingface เป็นแพลตฟอร์ม (ระบบที่ทุกคนเข้าใช้ร่วมกัน แทนที่จะมีคนกลุ่มเดียวที่เข้าใช้ได้) ที่มุ่งเน้นการพัฒนาและแบ่งปันโมเดลการประมวลผลภาษาธรรมชาติที่มีความสามารถหลากหลาย กล่าวคือนักพัฒนาสามารถนำโมเดลที่ตนสร้างขึ้นมาฝากไว้บนแพลตฟอร์มนี้ เพื่อให้ผู้ใช้คนอื่นเข้ามาโหลดเพื่อนำไปใช้ได้อย่างสะดวก ซึ่งหน้าที่ของแพลตฟอร์มคืออำนวยความสะดวกให้ทั้งสองฝ่ายโดยไม่ต้องพัฒนาโมเดลทั้งหมดเอง  

```{margin} คำศัพท์
แพลตฟอร์ม (platform) 
```

แพลตฟอร์ม huggingface อยู่บนเว็บไซต์ https://huggingface.co  และมาพร้อมกับไลบรารีที่เรียกว่า transformers ซึ่งเป็นไลบรารีไพทอนที่ช่วยโหลดโมเดลมาจากแพลตฟอร์ม เพื่อนำไปพัฒนาเป็นระบบอื่น ๆ ในภาษาไพทอนได้อย่างสะดวก huggingface รวมโมเดลที่ถูกพัฒนามาแล้วหลากหลายรุ่นที่สามารถนำไปใช้ในงานต่าง ๆ ได้ทันที ถ้าหากเราค้นหาโมเดล sentiment analysis บนแพลตฟอร์ม ก็จะพบว่ามีโมเดลให้เลือกกว่า 100 โมเดล เราจำเป็นต้องดูว่าโมเดลใดรองรับภาษาที่เราต้องการวิเคราะห์ รวมถึงตรวจสอบอีกว่าความแม่นยำที่คาดหวังได้คือเท่าไร ผู้ที่นำโมเดลมาฝากในแพลตฟอร์มนี้มักจะมีเขียนไว้อย่างละเอียดถึงกระบวนการพัฒนาโมเดล เหมาะกับข้อมูลประเภทใด (เช่น ทวีต รีวิว หรือข่าวการเงิน) และความแม่นยำตามที่ได้ประเมินสิทธิภาพไว้ 

เริ่มต้นจากการติดตั้งไลบรารีด้วยคำสั่ง `pip install transformers` วิธีการใช้ไลบรารีนี้แบบพื้นฐานที่สุด ทำได้โดยการใช้ฟังก์ชัน `pipeline` ในการโหลดโมเดลและปรับการใช้งาน โดยที่เราจะต้องทราบชื่อโมเดลที่ต้องการใช้ 
โมเดลที่แนะนำในปัจจุบัน (มิถุนายน 2567) สำหรับการวิเคราะห์อารมณ์ความรู้สึกภาษาไทยคือ `poom-sci/WangchanBERTa-finetuned-sentiment` ซึ่งเรียนรู้การวิเคราะห์มาจากชุดข้อมูลที่เป็นรีวิว รวมกันกับข้อมูลที่เป็นทวีต ทำให้เหมาะกับการวิเคราะห์เบื้องต้น ไม่ได้เฉพาะเจาะจงกับข้อมูลแหล่งใดแหล่งหนึ่ง 

```python
# Use a pipeline as a high-level helper
from transformers import pipeline
pipe = pipeline("text-classification", model="poom-sci/WangchanBERTa-finetuned-sentiment")
pipe(["เนื้อครีมลื่นไม่เหนียว", "กระปุกสีขาว ฝาเกลียว", "เปิดออกมาแล้วกลิ่นค่อนข้างแรง"])
```

ผลลัพธ์ที่ได้ออกมาคือ ลิสต์ของดิกชันนารีที่ประกอบด้วยป้ายกำกับ (label) ที่มีค่าความน่าจะเป็นสูงสุดคือป้ายกำกับที่ และค่าความน่าจะเป็น (score) ที่ได้ ดังนี้

```python
[{'label': 'pos', 'score': 0.6775356531143188},
 {'label': 'neu', 'score': 0.9761248230934143},
 {'label': 'neg', 'score': 0.7795576453208923}]
```

ภาษาไทยทีโมเดลอีกตัวหนึ่งซึ่งสร้างขึ้นมาเพื่อข้อมูลที่มาจากเอกสารทางการเงิน เช่น รายงานผลประกอบการ หรือข่าวทางการเงิน โมเดลตัวนี้ชื่อว่า `nlp-chula/augment-sentiment-finnlp-th` บนแพลต์ฟอร์ม huggingface เราสามารถนำมาใช้งานได้ด้วยคำสั่งเดียวกันเพียงแต่เปลี่ยนชื่อโมเดลที่ใช้ 

```python
pipe_finance_news = pipeline("text-classification", model="nlp-chula/augment-sentiment-finnlp-th")
pipe_finance_news([
    "บริษัทยังคงดำเนินนโยบายการขยายเครือข่ายสาขาอย่างต่อเนื่อง", 
     "ในช่วงครึ่งปีหลัง บริษัทเน้น เรื่องการเร่งรัดติดตามทวงถามหนี้เพิ่มมากขึ้น",
    "การแพร่ระบาดของไวรัสโควิด-19 ระลอกใหม่เป็นปัจจัย สาคัญที่กดดันต่อการบริโภคภาคเอกชน และการท่องเที่ยว",
    "ธนาคารมีสินทรัพย์สภาพคล่องประมาณ 47,272 ล้านบาท ลดลงจานวน 2,424 ล้านบาท ",
    "ธุรกิจกองทุนสํารองเลี้ยงชีพของ บลจ. ทิสโก้ยังคงเติบโตกว่าร้อยละ 11.8 เทียบกับ อุตสําหกรรมที่เติบโตลดลงเหลือเพียงร้อยละ 2.1 จากปี 2562"])
```

ผลลัพธ์ที่ได้ออกมาคือ 

```python
[{'label': 'Neutral', 'score': 0.6528419256210327},
 {'label': 'Neutral', 'score': 0.937697172164917},
 {'label': 'Negative', 'score': 0.9270865321159363},
 {'label': 'Negative', 'score': 0.8279978036880493},
 {'label': 'Positive', 'score': 0.5197721719741821}]
 ```

ข้อควรระวังอย่างหนึ่งของการวิเคราะห์อารมณ์ความรู้สึก คือการเลือกใช้โมเดลให้เหมาะสมกับขอบเขตเนื้อหา (domain) ของข้อมูลที่เราต้องการวิเคราะห์  ถ้าหากชุดข้อมูลที่ใช้พัฒนาโมเดลมีขอบเขตเนื้อหาที่ไม่ตรงกับ ขอบเขตเนื้อหาของข้อมูลที่เราต้องการวิเคราะห์ อาจจะทำให้ความแม่นยำลดลง หรือผลลัพธ์ที่ได้ไม่เป็นที่พอใจ สมมติว่าโมเดลการวิเคราะห์อารมณ์ความรู้สึก (sentiment analysis) ถูกพัฒนามาด้วยข้อมูลรีวิวภาพยนตร์ ซึ่งข้อมูลนี้มีลักษณะเฉพาะ เช่น คำศัพท์และรูปแบบการใช้ภาษาที่เกี่ยวข้องกับการรีวิวภาพยนตร์ หากเรานำโมเดลนี้ไปใช้วิเคราะห์ความคิดเห็นเกี่ยวกับการเงินการลงทุน  โมเดลก็จะมีความแม่นยำลดลง เพราะลักษณะทางภาษาหรือขอบเขตเนื้อหาของข้อมูลรีวิวสมาร์ทโฟนอาจมีความแตกต่างจากข้อมูลความเห็นเกี่ยวกับการเงินการลงทุน  เรียกอีกอย่างว่า ปัญหาการกระจายตัวแบบออกนอกขอบเขต (out-of-distribution) ซึ่งเป็นปัญหาที่สำคัญในการใช้โมเดลการเรียนรู้ของเครื่องในงานปัญญาประดิษฐ์ {cite}`ood2023`

ในตัวอย่างการใช้ huggingface ข้างต้น เราได้ลองใช้โมเดลภาษาไทยสองตัวที่ถูกพัฒนาขึ้นมาจากชุดข้อมูลคนละขอบเขตกัน ตัวแรกสร้างขึ้นเพื่อใช้วิเคราะห์รีวิวและทวีต และโมเดลตัวที่สองสร้างขึ้นเพื่อใช้วิเคราะห์ข้อความที่ขอบเขตเนื้อหาคือการเงิน และผลประกอบการของบริษัท 
หากเราใช้โมเดลตัวแรกมาวิเคราะห์ข้อความที่พูดถึงเรื่องการเงิน จะได้ผลดังนี้

```python
[{'label': 'neu', 'score': 0.9173294305801392},
 {'label': 'neu', 'score': 0.9335467219352722},
 {'label': 'neu', 'score': 0.8750076293945312}, # ที่ถูกต้องคือ 'neg'
 {'label': 'neu', 'score': 0.9450038075447083}, # ที่ถูกต้องคือ 'neg'
 {'label': 'neu', 'score': 0.9596440196037292}] # ที่ถูกต้องคือ 'pos'
```

หากเราใช้โมเดลตัวที่สองมาวิเคราะห์ที่เป็นรีวิวจะได้ผลดังนี้

```python
[{'label': 'Neutral', 'score': 0.6709030270576477}, # ที่ถูกต้องคือ 'Positive'
 {'label': 'Positive', 'score': 0.5924004316329956},# ที่ถูกต้องคือ 'Neutral'
 {'label': 'Neutral', 'score': 0.8645471930503845}]# ที่ถูกต้องคือ 'Negative'
```

เมื่อโมเดลทั้งสองตัวถูกนำไปใช้กับข้อมูลที่ไม่ตรงกับขอบเขตเนื้อหาที่ใช้พัฒนา ความแม่นยำลดลง และผลลัพธ์ที่ได้ไม่เป็นที่พอใจ ดังนั้นการเลือกใช้โมเดลที่เหมาะสมกับข้อมูลที่ต้องการวิเคราะห์เป็นสิ่งสำคัญ โดยทั่วไปแล้วโมเดลที่ได้รับความนิยมบนแพลตฟอร์ม huggingface มักจะระบุว่าโมเดลถูกพัฒนาขึ้นมาด้วยชุดข้อมูลอะไร ที่มาของข้อความอยู่ในขอบเขตเนื้อหาอะไร เพื่อผู้ใช้จะได้นำไปประยุกต์ใช้ได้อย่างเหมาะสมกับข้อมูลที่ต้องการวิเคราะห์ 

## โมเดลหัวข้อ (Topic Model)

โมเดลหัวข้อเป็นหนึ่งในโมเดลที่เป็นที่นิยมในการวิเคราะห์หาหัวข้อที่อยู่ในชุดเอกสาร จุดประสงค์ของโมเดลหัวข้อคือระบุและจัดกลุ่มข้อความที่มีเนื้อหาคล้ายคลึงกัน การทำงานของโมเดลหัวข้อนั้นคล้ายกับการวิเคราะห์ความถี่ของคำ (word frequency analysis) เพียงแต่ว่าเราไม่ต้องจัดคำที่ปรากฏบ่อยให้เป็นหมวดหมู่ด้วยตัวเอง โมเดลหัวข้อที่ได้รับความนิยมและใช้กันอย่างแพร่หลายคือ Latent Dirichlet Allocation (LDA) ซึ่งเป็นเทคนิคที่ช่วยในการหาหัวข้อทั้งหมดที่อยู่ในชุดเอกสารโดยการวิเคราะห์ความน่าจะเป็นของการเกิดร่วมกันของคำในเอกสาร นอกจากนั้นแล้วโมเดล LDA ยังช่วยในการระบุหัวข้อหลักในเอกสารแต่ละเอกสารอีกด้วย  ตัวอย่างเช่น หากเรามีชุดเอกสารที่เกี่ยวกับข่าวต่างประเทศ โมเดล LDA อาจช่วยในการระบุหัวข้อเช่น "การเมือง", "เศรษฐกิจ", และ "กีฬา" ทำให้เราสามารถจัดหมวดหมู่ข้อมูลและวิเคราะห์เชิงลึกได้ง่ายขึ้น นอกจากนี้ โมเดลหัวข้อยังสามารถนำไปใช้ในงานต่างๆ เช่น การจัดหมวดหมู่บทความในเว็บไซต์ข่าว การวิเคราะห์เอกสารทางวิชาการเพื่อค้นหาประเด็นวิจัยสำคัญ และการประมวลผลข้อความจากโซเชียลมีเดียเพื่อหาแนวโน้มและความสนใจของผู้ใช้งาน เป็นต้น

โมเดล LDA มีรายละเอียดทางคณิตศาสตร์ที่ซับซ้อน ซึ่งอยู่นอกเหนือขอบเขตของหนังสือเล่มนี้ {cite}`blei2003lda` อย่างไรก็ตาม เราสามารถเข้าใจหลักการทำงานของ LDA ได้อย่างง่าย ๆ โดยไม่ต้องลงลึกในรายละเอียดสมการของ LDA มากนัก LDA ทำงานโดยการหาคำที่เกิดขึ้นร่วมกันบ่อย ๆ ในเอกสารเดียวกัน สมมติว่าชุดเอกสารที่เราวิเคราะห์มีบทความข่าวทั้งหมด 10 ข่าว ซึ่งอาจจะประกอบไปด้วยข่าวการเมือง ข่าวเศรษฐกิจ และข่าวกีฬา แต่เราไม่ทราบมาก่อนเลยว่าข่าวทั้ง 10 ข่าวนี้จัดกลุ่มเป็นข่าวประเภทใดได้บ้าง แต่เราพอจะคาดเดาได้ว่าน่าจะมีสัก 3 ประเภทด้วยกัน 

ข้อสังเกตที่สำคัญที่ทำให้ LDA ทำงานได้อย่างมีประสิทธิภาพคือ เราสามารถจัดหัวข้อตามคีย์เวิร์ดได้ และคีย์เวิร์ดที่มาจากหัวข้อเดียวกันเหล่านี้มักพบเห็นพร้อมกันในเอกสารเดียวกัน เช่น  ข่าวการเมืองมักจะมีคำเฉพาะเจาะจงที่เกี่ยวข้องกับการเมือง เช่น *นายกรัฐมนตรี รัฐบาล รัฐสภา เลือกตั้ง ประกาศ ลงมติ* และข่าวการเมืองหนึ่งข่าวมักจะใช้คำเหล่านี้หลาย ๆ คำในข่าวเดียวกัน  ข่าวเศรษฐกิจมักจะมีคำเฉพาะเจาะจงที่เกี่ยวข้องกับเศรษฐกิจ เช่น *ธุรกิจ ชะลอตัว ลด การลงทุน ดอกเบี้ย อัตรา เศรษฐกิจ* ข่าวการเมืองก็อาจจะใช้คีย์เวิร์ดเหล่านี้บ้าง เพราะว่าข่าวการเมืองก็อาจจะพูดถึงเศรษฐกิจด้วย แต่ว่าไม่ใช่ข่าวการเมืองทุกข่าวจะพูดถึงเศรษฐกิจ เพราะเหตุนี้ผลนี้เอง โมเดล LDA จึงทำงานโดยการตรวจหาคำที่เกิดร่วมกันบ่อย ๆ ในเอกสารเดียวกัน และรวบรวมรายการของคำเหล่านี้รวมกันเป็นหัวข้อ เพราะฉะนั้น*หัวข้อ* ที่ได้จาก LDA คือรายการของคำและแต่ละคำมีค่าน้ำหนักบ่งบอกถึงความสำคัญ 

หากเราฝึกโมเดล LDA บนเอกสารที่เป็นข่าว 10 ข่าวและกำหนดให้แยกออกมาเป็น 3 หัว หัวข้อที่ได้ออกมาเป็นผลลัพท์อาจมีลักษณะดังนี้ 

| หัวข้อ | คีย์เวิร์ด | 1 | 2 | 3 | 4 | 5 |
|---|-- |--|---|---|---|---|
| 1 |  |นายกรัฐมนตรี |รัฐบาล | รัฐสภา | ลงมติ | กรรมการ | 
|   | น้ำหนัก    | 0.3 | 0.2 | 0.1 | 0.1 | 0.05 |
| 2 | | ธุรกิจ | ชะลอตัว | ลด | การลงทุน | ดอกเบี้ย |
|   | น้ำหนัก    | 0.25 | 0.2 | 0.2 | 0.1 | 0.1 |
| 3 | | ฟุตบอล | บาสเกตบอล | กรรมการ | ชนะ | คะแนน |
|   | น้ำหนัก     | 0.1 | 0.1 | 0.1 | 0.05 | 0.025 |

หัวข้อที่ได้จาก LDA มักจะเรียกอีกอย่างหนึ่งว่าคำสำคัญของหัวข้อ (topic key) ซึ่งเราจะต้องอ่านและตีความว่าหัวข้อแต่ละหัวข้อเกี่ยวกับอะไรโดยการเชื่อมโยงหาธีมด้วยตัวเอง ตัวโมเดลเองไม่ได้ประกาศชัดเจนว่าหัวข้อที่ 1 2 หรือ 3 เกี่ยวกับอะไร จากตัวอย่างข้างต้นเราตีความว่าหัวข้อที่ 1 เกี่ยวกับการเมือง หัวข้อที่ 2 เกี่ยวกับเศรษฐกิจ และหัวข้อที่ 3 เกี่ยวกับกีฬา 

ในกระบวนการวิเคราะห์เดียวกัน LDA ยังจัดกลุ่มเอกสารโดยดูว่ามีเอกสารแต่ละเอกสารมีคำจากหัวข้อใดเป็นสัดส่วนเท่าไร ผลลัพธ์คือสัดส่วนของหัวข้อแต่ละหัวข้อในแต่ละเอกสาร ซึ่งเราจะเรียกว่าตาราง doc-topic เพราะว่าตารางผลลัพธ์จะมีเอกสารอยู่ที่แถวแต่ละแถว และมีหัวข้ออยู่ใน เช่น ถ้าหากเรามี 10 เอกสาร 

| เอกสาร | หัวข้อ 1 | หัวข้อ 2 | หัวข้อ 3 |
|---|---|---|---|
| 1 | 0.05 | 0.05 | 0.9 |
| 2 | 0.1 | 0.8 | 0.1 |
| 3 | 0.8 | 0.1 | 0.1 |
| 4 | 0.0 | 0.05 | 0.95 |
| 5 | 0.5 | 0.4 | 0.1 |
| 6 | 0.7 | 0.3 | 0.0 |
| 7 | 0.1 | 0.8 | 0.1 |
| 8 | 0.9 | 0.05 | 0.05 |
| 9 | 0.95 | 0.025 | 0.025 |
| 10 | 0.1 | 0.9 | 0.0 |

จากตัวอย่างผลลัพธ์ข้างต้น เอกสารที่ 3 5 7 และ 9 น่าจะเป็นข่าวการเมือง เนื่องจากมีสัดส่วนของหัวข้อ 1 มากที่สุด แต่เราสังเกตว่าเอกสารที่ 5 มีสัดส่วนของหัวข้อ 1 และ 2 พอ ๆ กัน ซึ่งอาจจะตีความได้ว่าเป็นเอกสารที่พูดถึงทั้งการเมืองและเศรษฐกิจ 
ส่วนเอกสารที่ 2 8 และ 10 น่าจะเป็นข่าวเศรษฐกิจ เนื่องจากมีสัดส่วนของหัวข้อ 2 มากที่สุด และเอกสารที่ 1 และ 4 น่าจะเป็นข่าวกีฬา เนื่องจากมีสัดส่วนของหัวข้อ 3 มากที่สุด  

สรุปแล้ว LDA ช่วยในการหาหัวข้อที่อยู่ในชุดเอกสาร และจัดกลุ่มเอกสารตามหัวข้อที่พบในเอกสารไปในคราวเดียวกัน โดยที่เราไม่ต้องระบุหัวข้อล่วงหน้า แต่เราต้องระบุจำนวนหัวข้อที่เราคิดว่าเหมาะสมกับชุดเอกสาร 
เมื่อฝึกโมเดลเสร็จแล้วเราสามารถตีความหัวข้อที่ได้จาก LDA ได้อย่างง่าย ๆ โดยการดูคีย์เวิร์ดที่เกี่ยวข้องกับหัวข้อนั้น และดูสัดส่วนของหัวข้อในแต่ละเอกสาร 

## ไลบรารี gensim สำหรับการฝึกโมเดล LDA

ไลบรารี gensim เป็นไลบรารีภาษาไพทอนที่เป็นที่นิยมมากสำหรับการใช้โมเดล LDA ไลบรารีนี้ทำให้การฝึกและใช้งานโมเดล LDA เป็นเรื่องง่ายและมีประสิทธิภาพ ไลบรารีนี้มีฟังก์ชันอำนวยความสะดวกในการเตรียมข้อมูลและการฝึกโมเดล gensim มีเครื่องมือที่ช่วยในการแปลงเอกสารให้เป็นเวกเตอร์และสร้างดิกชันนารีของคำ (dictionary) ซึ่งเป็นขั้นตอนที่สำคัญก่อนที่จะเริ่มฝึกโมเดล LDA นอกจากนี้ gensim ยังมีฟังก์ชันที่ช่วยในการปรับพารามิเตอร์ของโมเดลเพื่อให้ได้ผลลัพท์ที่ออกมามีประโยชน์ที่สุด 

ขั้นตอนแรกของการฝึกโมเดลคือการเปลี่ยนข้อมูลให้อยู่ในรูปของลิสต์ของเอกสาร ซึ่งเอกสารหนึ่งเอกสารจะต้องถูกเก็บอยู่ในรูปลิสต์ของคำที่ปราศจากคำหยุด เพราะฉะนั้นเราจะต้องทำการตัดคำ สมมติว่าเราต้องการวิเคราะห์ข้อมูลรีวิวสายการบิน ที่อยู่ในไฟล์ zip ที่มีชื่อไฟล์ว่า Airline_review.csv.zip ซึ่งมีคอลัมน์ *Review* ที่เก็บข้อมูลรีวิว ขั้นตอนแรกเราจะทำการอ่านข้อมูลจากไฟล์ zip และทำการตัดคำด้วยไลบรารี nltk และเก็บข้อมูลในรูปลิสต์ของคำ โดยเราจะใช้ฟังก์ชัน `clean` ที่เราได้สร้างขึ้นมาเพื่อทำความสะอาดข้อมูล และการตัดคำ 

```python
import pandas as pd
import re
from zipfile import ZipFile

zip_file_path = 'Airline_review.csv.zip'

# อ่าน csv ที่อยู่ในไฟล์ zip โดยที่ไม่ต้อง unzip ออกมาใส่เครื่องเพิ่ม
with ZipFile(zip_file_path, 'r') as zip_ref:
    with zip_ref.open('Airline_review.csv') as file:
        df = pd.read_csv(file)

def clean(text):
    # ลบตัวเลขและเครื่องหมายวรรคตอน
    text = re.sub('[^a-zA-Z\s]', '', text).lower()
    # ตัดคำ
    words = nltk.word_tokenize(text)
    # ต้องระบุคำหยุดใน stop_word_set ก่อน
    words = [x for x in words if x not in stop_word_set]
    return words

reviews = df['Review'].apply(clean).tolist()
```

โค้ดข้างต้นเรารันฟังก์ชัน `clean` ลงไปบนคอลัมน์ *Review* ที่อยู่ในดาตาเฟรม และเก็บใส่ตัวแปรชื่อว่า `reviews` ซึ่งมีโครงสร้างเป็นลิสต์ของลิสต์ของสตริง (คำ) ข้อมูลที่อยู่ในรูปนี้เป็นรูปแบบที่พร้อมใช้งานกับไลบรารี gensim และเราสามารถนำไปใช้ในการฝึกโมเดล LDA ได้เลย

```python
# use gensim to create a dictionary and a corpus
from gensim import corpora, models

dictionary = corpora.Dictionary(reviews)
corpus = [dictionary.doc2bow(review) for review in reviews]
lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, 
                            passes=5, alpha='auto', eta='auto',  random_state=10)
```

ในโค้ดข้างต้นเราใช้คลาส `gensim.corpora.Dictionary` ในการสร้างดิกชันนารีของคำ และใช้เมท็อด `doc2bow` ในการแปลงข้อมูลให้อยู่ในรูปของลิสต์ของตัวเลข และเก็บใส่ตัวแปร `corpus` ซึ่งเป็นลิสต์ของลิสต์ของตัวเลข หลังจากนั้นเราใช้คลาส `gensim.models.LdaModel` ในการฝึกโมเดล LDA พารามิเตอร์ที่สำคัญที่สุดคือ จำนวนหัวข้อ (`num_topics`) ที่เราต้องการให้โมเดลหา ส่วนพารามิเตอร์อื่น ๆ ที่เกี่ยวข้องกับการฝึกโมเดลสามารถตั้งตามที่เห็นตัวอย่างได้เลย เช่น จำนวนรอบการฝึก (`passes`) เป็น 5 และการกำหนดค่าพารามิเตอร์ `alpha` และ `eta` โดยเราสามารถใช้ค่า `auto`  ส่วน `random_state` สามารถตั้งค่าเป็นอะไรก็ได้ การฝึกโมเดล LDA มีการต้องสุ่มค่าเริ่มต้นการฝึก เพราะฉะนั้นหากเราฝึกโมเดลใหม่อีกครั้งโดยใช้ค่า `random_state` เดียวกันก็จะได้ผลออกมาเหมือนเดิมทุกอย่าง หากไม่ระบุเลยแล้วกดรันอีกครั้งหนึ่งผลจะออกมาแตกต่าง 

 หลังจากฝึกโมเดลเสร็จเราสามารถดูหัวข้อที่ได้จากโมเดลได้ด้วยเมท็อด `show_topic` ซึ่งจะคืนค่าเป็นลิสต์ของทูเปิล (คำ, ค่าน้ำหนัก) เรียงลำดับตั้งแต่มากไปน้อย ตัวอย่างเช่น หากเราเรียก `lda_model.show_topic(0)` จะได้ผลลัพธ์ดังนี้

 ```python
 [('flight', 0.035730693),
 ('good', 0.017981019),
 ('food', 0.016260142),
 ('crew', 0.016218152),
 ('service', 0.01401287),
 ('cabin', 0.013283503),
 ('time', 0.011989626),
 ('seats', 0.00853041),
 ('friendly', 0.008275962),
 ('staff', 0.00817701)]
 ```

หากต้องการดูทุกหัวข้อโดยดูเฉพาะคำที่มีค่าน้ำหนักสูงสุด 10 คำแรก gensim ไม่ได้มีเมท็อดที่แสดงผลออกมาให้ดูง่าย เราจึงต้องเขียนลูปดังนี้

```python
for i in range(lda_model.num_topics):
    keyword_list = [w[0] for w in lda_model.show_topic(i)]
    print (f'หัวข้อ {i}: {" ".join(keyword_list)}')
```
ผลลัพธ์ที่ได้จะมีลักษณะดังนี้

```
หัวข้อ 0: flight good food crew service cabin time seats friendly staff
หัวข้อ 1: luggage bag baggage pay kg extra carry bags checked wow
หัวข้อ 2: de la que volotea el en las con por un
หัวข้อ 3: airline air flights airlines service fly time like flying low
หัวข้อ 4: seat seats extra flight row paid get front plane back
หัวข้อ 5: luggage baggage lost bag bags claim days manila arrived suitcase
หัวข้อ 6: ticket pay check service online customer airline website said told
หัวข้อ 7: johannesburg china kuala lumpur test town cape klm chinese shanghai
หัวข้อ 8: flight us plane airport boarding staff time gate passengers minutes
หัวข้อ 9: flight airline hours get us service customer day time would
```

จากนั้นเราต้องนำหัวข้อแต่ละหัวข้อมาตีความต่อ

- หัวข้อ 0: บริการบนเครื่องบิน
- หัวข้อ 1: การจ่ายค่าบริการเกี่ยวกับน้ำหนักกระเป๋าที่สามารถนำขึ้นเครื่องได้
- หัวข้อ 2: เป็นคำศัพท์ภาษาสเปนที่ส่วนใหญ่เป็นคำหยุด ทำให้เห็นว่าข้อมูลเราอาจจะมีข้อมูลที่ไม่เป็นภาษาสเปนติดมาด้วย เราควรจะกลับไปกรองออกก่อนให้เรียบร้อย
- หัวข้อ 3: บริการของสายการบิน
- หัวข้อ 4: ที่นั่งบนเครื่องบิน
- หัวข้อ 5: การสูญหายของกระเป๋า
- หัวข้อ 6: การซื้อตั๋วและการบริการลูกค้าออนไลน์ ผ่านหน้าเว็บไซต์
- หัวข้อ 7: สถานที่เป็นปลายทางการบิน
- หัวข้อ 8: ประสบการณ์การเช็คอินและขึ้นเครื่อง
- หัวข้อ 9: ระยะเวลาที่แผนกบริการลูกค้าใช้ในการตอบกลับลูกค้า

ถ้าหากว่าผลลัพธ์ออกมาไม่ดีเท่าทีควร มีทางแก้อยู่สองทางด้วยกัน 

1. ลองเปลี่ยนจำนวนหัวข้อ ถ้าจำนวนหัวข้อมากเกินไป เราจะพบเห็นหัวข้อที่ซ้ำ ๆ กัน ถ้าจำนวนหัวข้อน้อยเกินไป เราจะพบเห็นหัวที่มีคำที่ไม่เกี่ยวข้องกันอยู่ด้วย ดังนั้นเราควรจะลองเปลี่ยนจำนวนหัวข้อใหม่อีกครั้ง

2. ทำความสะอาดข้อมูลให้ดีขึ้น หากเจอคำที่เกิดขึ้นในหลาย ๆ หัวข้อที่ไม่ได้มีประโยชน์ในการตีความ ทำความเข้าใจหัวข้อ อาจจะสกัดเอาคำเหล่านั้นออกไป เช่น ในตัวอย่างข้างต้น เราอาจจะเอาคำว่า *flight* และ *flights* ออกไปเนื่องจาก เราทราบอยู่แล้วว่าข้อมูลเกี่ยวกับสายการบิน คำว่า *flight* ย่อมปรากฏอยู่ในทุกรีวิว และไม่มีประโยชน์ในการตีความหัวข้อ

## สรุป

ในบทนี้เราเรียนรู้เทคนิควิธีการประมวลผลภาษาธรรมชาติที่ใช้บ่อย ๆ ได้แก่ การแท็กชนิดของคำ การตรวจจับเอนทิตี การวิเคราะห์อารมณ์ความรู้สึก การวิเคราะห์หัวข้อที่ปรากฏในชุดเอกสาร ในปัจจุบันมีไลบรารีที่สามารถดึงโมเดลต่าง ๆ ลงมาใช้ในภาษาไพทอนได้สะดวก เนื่องจากมีซอฟต์แวร์ที่เป็นโอเพนซอร์สมากมายที่ได้รับการพัฒนาอย่างต่อเนื่อง ทำให้สามารถทำการประมวลผลภาษาธรรมชาติได้ง่ายขึ้น หรือสามารถเทรนโมเดลบนชุดข้อมูลของเราเองอย่างการทำโมเดลหัวข้อ 

## อ้างอิง
```{bibliography}
:filter: docname in docnames
:style: plain

```