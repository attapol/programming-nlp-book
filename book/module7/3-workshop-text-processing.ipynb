{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# แบบฝึกหัด: การประมวลผลข้อความขั้นพื้นฐาน"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## สรุปคำสั่งที่ควรทราบ\n",
    "\n",
    "### การติดตั้งไลบรารี pythainlp attacut และ nltk ก่อนใช้งาน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pythainlp\n",
    "!pip install attacut\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### การตัดคำภาษาไทย"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ข้างนอก', 'สุกใส', ' ', 'ข้างใน', 'ต๊ะติ๊ง', 'โหน่ง']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pythainlp\n",
    "import attacut\n",
    "pythainlp.word_tokenize('ข้างนอกสุกใส ข้างในต๊ะติ๊งโหน่ง')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ข้าง', 'นอก', 'สุกใส', ' ', 'ข้าง', 'ใน', 'ต๊ะติ๊งโหน่ง']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythainlp.word_tokenize('ข้างนอกสุกใส ข้างในต๊ะติ๊งโหน่ง', engine='attacut')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ตัดคำภาษาอังกฤษ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COVID-19',\n",
       " ',',\n",
       " 'which',\n",
       " 'caused',\n",
       " 'problems',\n",
       " 'worldwide',\n",
       " ',',\n",
       " 'is',\n",
       " 'still',\n",
       " 'a',\n",
       " 'problem',\n",
       " 'in',\n",
       " 'the',\n",
       " 'U.S.A.',\n",
       " 'until',\n",
       " 'today',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tokenize.word_tokenize('COVID-19, which caused problems worldwide, is still a problem in the U.S.A. until today.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ตัดประโยคภาษาไทย"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ระฆังดี ', 'ถึงแม้คนไม่ตีก็ดัง ', 'ระฆังไม่ดีไม่ตีก็ไม่ดัง']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythainlp.sent_tokenize('ระฆังดี ถึงแม้คนไม่ตีก็ดัง ระฆังไม่ดีไม่ตีก็ไม่ดัง')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ตัดประโยคภาษาอังกฤษ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Punkt knows that the periods in Mr. Smith and Johann S. Bach are not sentence boundaries.',\n",
       " 'But he was with Mrs.',\n",
       " 'Bond that week.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize('Punkt knows that the periods in Mr. Smith and Johann S. Bach are not sentence boundaries.  But he was with Mrs. Bond that week.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### โหลดคำหยุดภาษาไทย"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ข้างนอก', 'สุกใส', ' ', 'ข้่าง', 'ต๊ะติ๊ง', 'โหน่ง']\n"
     ]
    }
   ],
   "source": [
    "tokens = pythainlp.word_tokenize('ข้างนอกสุกใส ข้่างในต๊ะติ๊งโหน่ง')\n",
    "stopset = set(pythainlp.corpus.thai_stopwords())\n",
    "tokens_no_stopwords = [t for t in tokens if t not in stopset]\n",
    "print(tokens_no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### โหลดคำหยุดภาษาอังกฤษ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID-19', ',', 'caused', 'problems', 'worldwide', ',', 'still', 'problem', 'U.S.A.', 'today', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize('COVID-19, which caused problems worldwide, is still a problem in the U.S.A. until today.')\n",
    "stopset = set(nltk.corpus.stopwords.words('english'))\n",
    "tokens_no_stopwords = [t for t in tokens if t not in stopset]\n",
    "print(tokens_no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### กรองเอาเครื่องหมายวรรคตอนออก"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID-19', 'which', 'caused', 'problems', 'worldwide', 'is', 'still', 'a', 'problem', 'in', 'the', 'U.S.A.', 'until', 'today']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "patt = re.compile('[^ก-์0-9a-zA-Z]')\n",
    "tokens_no_punct = [t for t in tokens if not patt.match(t)]\n",
    "print(tokens_no_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## แบบฝึกหัดการวิเคราะห์ความถี่ของคำ\n",
    "เขียนฟังก์ชันที่นับจำนวนคำภาษาไทยจากข้อความที่เก็บอยู่ในสตริง `news_article` \n",
    "โดยให้ใช้ข่าวที่มาจาก `https://thestandard.co/us-prepares-to-offer-billionaire-tax/` หรือดาวน์โหลดจาก `https://github.com/attapol/programming-nlp-book/tree/main/module7/data/sample-news-article.txt` ได้โดยตรง หรือใช้ข้อความจากแหล่งอื่นที่ความยาวเกิน 1000 คำทดแทนได้\n",
    "\n",
    "1. สตริงที่ให้มีขนาดคลังศัพท์ (จำนวนคำแบบไม่นับคำซ้ำ) อยู่ที่เท่าไร\n",
    "2. คำใดพบบ่อยที่สุด 10 อันดับในสตริงที่ให้มา โดยที่ไม่ต้องกรองเอาคำหยุดออก\n",
    "3. ไบแกรมใดพบบ่อยที่สุด 10 อันดับในสตริงที่ให้มา โดยที่ไม่ต้องกรองเอาคำหยุดออก\n",
    "4. คำใดพบบ่อยที่สุด 10 อันดับในสตริงที่ให้มา โดยที่กรองเอาคำหยุดออก และเปรียบเทียบผล\n",
    "5. สร้างเมฆคำจากคำที่พบบ่อยที่สุด 40 อันดับ ให้ปรับขนาดของคำให้เท่ากับความถี่ของคำนั้นๆ ไลบรารีเมฆคำไม่มีฟอนต์ภาษาไทยมาให้ เพราะฉะนั้นเราต้องดาวน์โหลดฟอนต์ภาษาไทยมาวางที่โฟลเดอร์เดียวกัน และตั้งค่า `font_path=` ให้ชี้ไปยังไฟล์ที่เก็บฟอนต์ไทย"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
